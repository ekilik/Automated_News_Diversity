{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import spacy_stanza\n",
    "\n",
    "import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:58:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 19.6MB/s]                    \n",
      "2025-02-13 22:58:46 INFO: Loading these models for language: nl (Dutch):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | alpino          |\n",
      "| mwt       | alpino          |\n",
      "| pos       | alpino_charlm   |\n",
      "| lemma     | alpino_nocharlm |\n",
      "| depparse  | alpino_charlm   |\n",
      "| ner       | conll02         |\n",
      "===============================\n",
      "\n",
      "2025-02-13 22:58:46 INFO: Using device: cuda\n",
      "2025-02-13 22:58:46 INFO: Loading: tokenize\n",
      "2025-02-13 22:58:46 INFO: Loading: mwt\n",
      "2025-02-13 22:58:46 INFO: Loading: pos\n",
      "2025-02-13 22:58:46 INFO: Loading: lemma\n",
      "2025-02-13 22:58:47 INFO: Loading: depparse\n",
      "2025-02-13 22:58:47 INFO: Loading: ner\n",
      "2025-02-13 22:58:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# load nlp stanza\n",
    "nlp = spacy_stanza.load_pipeline(\"nl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Merge DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the all annotated NOS articles\n",
    "\n",
    "df = pd.read_csv('coded_df_actors_full.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read articles \n",
    "df_text = pd.read_csv('final_nosarticles.csv',\n",
    "                      sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "df_text['page_id'] = df_text['page_id'].astype(int)\n",
    "\n",
    "df_text = df_text[['page_id', 'Text']].drop_duplicates()\n",
    "\n",
    "# rename page_id to article_id\n",
    "df_text.rename(columns = {'page_id': 'article_id'}, inplace = True)\n",
    "\n",
    "# remove line break\n",
    "df_text['Text'] = df_text['Text'].str.replace('[LINE_BREAK]', '\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add text to the annotation df\n",
    "df = pd.merge(df, df_text, on = 'article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop if actor_type is Geopolitieke entiteit\n",
    "df = df[df.actor_type != 'Geopolitieke entiteit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles_df = df[['article_id', 'Text']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Named Entity Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:59:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 26.2MB/s]                    \n",
      "2025-02-13 22:59:34 INFO: Loading these models for language: nl (Dutch):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | alpino          |\n",
      "| mwt       | alpino          |\n",
      "| pos       | alpino_charlm   |\n",
      "| lemma     | alpino_nocharlm |\n",
      "| depparse  | alpino_charlm   |\n",
      "| ner       | conll02         |\n",
      "===============================\n",
      "\n",
      "2025-02-13 22:59:34 INFO: Using device: cuda\n",
      "2025-02-13 22:59:34 INFO: Loading: tokenize\n",
      "2025-02-13 22:59:34 INFO: Loading: mwt\n",
      "2025-02-13 22:59:34 INFO: Loading: pos\n",
      "2025-02-13 22:59:34 INFO: Loading: lemma\n",
      "2025-02-13 22:59:34 INFO: Loading: depparse\n",
      "2025-02-13 22:59:34 INFO: Loading: ner\n",
      "2025-02-13 22:59:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy_stanza.load_pipeline(\"nl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text_stanza(text):\n",
    "    # Create a Sentence object from the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Get the tagged spans\n",
    "    spans = doc.ents\n",
    "\n",
    "    # drop if entity label is not in ['ORG','PER']\n",
    "    spans = [span for span in spans if span.label_ in ['ORG', 'PER']]\n",
    "    \n",
    "    # Create a list of tuples containing the entity text and label\n",
    "    entities = [(ent.text, ent.label_) for ent in spans]\n",
    "\n",
    "    # Create an empty dictionary to store unique combinations\n",
    "    unique_entities_tags = {}\n",
    "\n",
    "    # Iterate through the list of named entity tag combinations\n",
    "    for entity, tag in entities:\n",
    "        # Check if the combination exists in the dictionary\n",
    "        if (entity, tag) not in unique_entities_tags:\n",
    "            # If it doesn't exist, add it to the dictionary\n",
    "            unique_entities_tags[(entity, tag)] = True\n",
    "\n",
    "    # Convert the keys of the dictionary back into a list\n",
    "    unique_entities = list(unique_entities_tags.keys())\n",
    "\n",
    "    return unique_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marion Koopmans', 'PER'),\n",
       " ('Wereldgezondheidsorganisatie', 'ORG'),\n",
       " ('Koopmans', 'PER'),\n",
       " ('Nieuwsuur', 'ORG'),\n",
       " ('RIVM', 'ORG'),\n",
       " ('Victor Lamme', 'PER'),\n",
       " ('Bas van den Putte', 'PER')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first text as test\n",
    "text = unique_articles_df['Text'].iloc[2]\n",
    "\n",
    "# tag the text\n",
    "unique_entities = tag_text_stanza(text)\n",
    "unique_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles_df['entities_stanza'] = unique_articles_df['Text'].apply(tag_text_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = unique_articles_df.explode('entities_stanza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4031, 5)\n"
     ]
    }
   ],
   "source": [
    "# 2. Split the 'entities' tuples into two separate columns: 'entity_name' and 'entity_type'\n",
    "df_exploded[['entity_name', 'entity_type']] = pd.DataFrame(df_exploded['entities_stanza'].tolist(), index=df_exploded.index)\n",
    "print(df_exploded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kabinet\n",
       "False    2572\n",
       "True     1459\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function to check if Text contains kabinet in its lower case form because kabinet cannot be found in the NER model\n",
    "def check_kabinet(text):\n",
    "    if 'kabinet' in text.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# if the text contains kabinet, then add entity_name as 'Het kabinet' and entity_type as 'ORG'\n",
    "df_exploded['kabinet'] = df_exploded['Text'].apply(check_kabinet)\n",
    "\n",
    "df_exploded['kabinet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get kabinet into a new df\n",
    "df_kabinet = df_exploded[df_exploded['kabinet'] == True]\n",
    "# drop entities_stanza and entity_name and entity_type\n",
    "df_kabinet = df_kabinet.drop(columns = ['entities_stanza', 'entity_name', 'entity_type'])\n",
    "\n",
    "df_kabinet['entity_name'] = 'Het kabinet'\n",
    "df_kabinet['entity_type'] = 'ORG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5490, 6)\n"
     ]
    }
   ],
   "source": [
    "# concat df_exploded and df_kabinet\n",
    "df_exploded = pd.concat([df_exploded, df_kabinet], axis = 0)\n",
    "print(df_exploded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop kabinet column\n",
    "df_exploded = df_exploded.drop(columns = 'kabinet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij', 'het', 'niet', 'zijn', 'is', 'was', 'op', 'aan', 'met', 'als', 'voor', 'had', 'er', 'maar', 'om', 'hem', 'dan', 'zou', 'of', 'wat', 'mijn', 'men', 'dit', 'zo', 'door', 'over', 'ze', 'zich', 'bij', 'ook', 'tot', 'je', 'mij', 'uit', 'der', 'daar', 'haar', 'naar', 'heb', 'hoe', 'heeft', 'hebben', 'deze', 'u', 'want', 'nog', 'zal', 'me', 'zij', 'nu', 'ge', 'geen', 'omdat', 'iets', 'worden', 'toch', 'al', 'waren', 'veel', 'meer', 'doen', 'toen', 'moet', 'ben', 'zonder', 'kan', 'hun', 'dus', 'alles', 'onder', 'ja', 'eens', 'hier', 'wie', 'werd', 'altijd', 'doch', 'wordt', 'wezen', 'kunnen', 'ons', 'zelf', 'tegen', 'na', 'reeds', 'wil', 'kon', 'niets', 'uw', 'iemand', 'geweest', 'andere']\n",
      "['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij', 'het', 'niet', 'zijn', 'is', 'was', 'op', 'aan', 'met', 'als', 'voor', 'had', 'er', 'maar', 'om', 'hem', 'dan', 'zou', 'of', 'wat', 'mijn', 'men', 'dit', 'zo', 'door', 'over', 'ze', 'zich', 'bij', 'ook', 'tot', 'je', 'mij', 'uit', 'der', 'daar', 'haar', 'naar', 'heb', 'hoe', 'heeft', 'hebben', 'deze', 'u', 'want', 'nog', 'zal', 'me', 'zij', 'nu', 'ge', 'geen', 'omdat', 'iets', 'worden', 'toch', 'al', 'waren', 'veel', 'meer', 'doen', 'toen', 'moet', 'ben', 'zonder', 'kan', 'hun', 'dus', 'alles', 'onder', 'ja', 'eens', 'hier', 'wie', 'werd', 'altijd', 'doch', 'wordt', 'wezen', 'kunnen', 'ons', 'zelf', 'tegen', 'na', 'reeds', 'wil', 'kon', 'niets', 'uw', 'iemand', 'geweest', 'andere', 'De', 'En', 'Van', 'Ik', 'Te', 'Dat', 'Die', 'In', 'Een', 'Hij', 'Het', 'Niet', 'Zijn', 'Is', 'Was', 'Op', 'Aan', 'Met', 'Als', 'Voor', 'Had', 'Er', 'Maar', 'Om', 'Hem', 'Dan', 'Zou', 'Of', 'Wat', 'Mijn', 'Men', 'Dit', 'Zo', 'Door', 'Over', 'Ze', 'Zich', 'Bij', 'Ook', 'Tot', 'Je', 'Mij', 'Uit', 'Der', 'Daar', 'Haar', 'Naar', 'Heb', 'Hoe', 'Heeft', 'Hebben', 'Deze', 'U', 'Want', 'Nog', 'Zal', 'Me', 'Zij', 'Nu', 'Ge', 'Geen', 'Omdat', 'Iets', 'Worden', 'Toch', 'Al', 'Waren', 'Veel', 'Meer', 'Doen', 'Toen', 'Moet', 'Ben', 'Zonder', 'Kan', 'Hun', 'Dus', 'Alles', 'Onder', 'Ja', 'Eens', 'Hier', 'Wie', 'Werd', 'Altijd', 'Doch', 'Wordt', 'Wezen', 'Kunnen', 'Ons', 'Zelf', 'Tegen', 'Na', 'Reeds', 'Wil', 'Kon', 'Niets', 'Uw', 'Iemand', 'Geweest', 'Andere']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elifk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "print(stopwords)\n",
    "\n",
    "# extend the stopwords with their title case form\n",
    "stopwords_title = [word.title() for word in stopwords]\n",
    "stopwords.extend(stopwords_title)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.isnull().sum()\n",
    "\n",
    "# see where the null values are\n",
    "df_exploded[df_exploded['entity_name'].isnull()]\n",
    "\n",
    "# drop if entity_name is null\n",
    "df_exploded = df_exploded.dropna(subset = ['entity_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to generate name variations (without stopword filtering)\n",
    "def generate_name_variations(name):\n",
    "    if not isinstance(name, str):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase and split into parts\n",
    "    # Keep only alphanumeric words (no symbols or numbers)\n",
    "    parts = re.findall(r'\\b\\w+\\b', name)\n",
    "    \n",
    "    # Remove stopwords from the parts\n",
    "    parts_filtered = [part for part in parts]\n",
    "    \n",
    "    variations = []\n",
    "    \n",
    "    if parts_filtered:\n",
    "        # Use the entire filtered name and the individual parts as variations\n",
    "        variations.append(' '.join(parts_filtered))  # Add filtered parts as a single variation\n",
    "        variations.extend(parts_filtered)  # Add individual parts as variations\n",
    "\n",
    "    return variations\n",
    "\n",
    "# Create a new column for name variations\n",
    "df_exploded['name_variations'] = df_exploded['entity_name'].apply(lambda x: generate_name_variations(x))\n",
    "\n",
    "# Drop from name_variations if instance of name_variations matches stopwords\n",
    "df_exploded['name_variations'] = df_exploded['name_variations'].apply(lambda x: [variation for variation in x if variation not in stopwords])\n",
    "df_exploded['name_variations'] = df_exploded['name_variations'].apply(lambda x: [variation for variation in x if re.match(r'\\b\\w+\\b', variation)])\n",
    "df_exploded['name_variations'] = df_exploded['name_variations'].apply(lambda x: [variation for variation in x if len(variation) > 1])\n",
    "# drop if variation is number\n",
    "df_exploded['name_variations'] = df_exploded['name_variations'].apply(lambda x: [variation for variation in x if not variation.isnumeric()])\n",
    "# keep only unique variations\n",
    "df_exploded['name_variations'] = df_exploded['name_variations'].apply(lambda x: list(set(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see where entity name has punctuation\n",
    "df_exploded[df_exploded['entity_name'].str.contains(r'[^\\w\\s]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to match whole words\n",
    "def is_full_word_match(variation, name):\n",
    "   \n",
    "    # Use word boundaries to ensure full word matching\n",
    "    return bool(re.search(r'\\b' + re.escape(variation) + r'\\b', name))\n",
    "\n",
    "# Function to map names within each article (case-insensitive, full-word match, and entity type check)\n",
    "def map_names_within_article(article_df):\n",
    "    name_mapping = {}\n",
    "    \n",
    "    # Step 1: Populate the name mapping with the longest canonical names\n",
    "    for index, row in article_df.iterrows():\n",
    "        variations = row['name_variations']\n",
    "        canonical_name = row['entity_name']\n",
    "        entity_type = row['entity_type']  # Get the entity type\n",
    "\n",
    "        for variation in variations:            \n",
    "            # Check both variation and entity_type match\n",
    "            for mapped_name, (current_canonical, current_type) in name_mapping.items():\n",
    "                if is_full_word_match(variation, mapped_name) or is_full_word_match(mapped_name, variation):\n",
    "                    if entity_type == current_type:  # Ensure entity types match\n",
    "                        if len(canonical_name) > len(current_canonical):\n",
    "                            name_mapping[mapped_name] = (canonical_name, entity_type)\n",
    "                    break\n",
    "            else:\n",
    "                name_mapping[variation] = (canonical_name, entity_type)\n",
    "\n",
    "    # Step 2: Replace entity names based on the canonical mapping (with full-word and entity-type check)\n",
    "    article_df['entity_name_new'] = article_df.apply(\n",
    "        lambda row: next(\n",
    "            (canonical for variation, (canonical, type_) in name_mapping.items()\n",
    "             if (is_full_word_match(variation, row['entity_name']) or is_full_word_match(row['entity_name'], variation)) \n",
    "             and type_ == row['entity_type']), \n",
    "            row['entity_name']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    return article_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_test = df_exploded.groupby(['article_id', 'entity_type'], group_keys=False).apply(map_names_within_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5483, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(552, 7)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see where entity_name and entity_name_new are different\n",
    "differences = df_exploded_test[df_exploded_test['entity_name'] != df_exploded_test['entity_name_new']]\n",
    "differences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see in differences the entity_type ORG for check\n",
    "# differences[differences['entity_type'] == 'ORG'][['entity_name', 'entity_name_new']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see in differences the entity_type PER for check\n",
    "# differences[differences['entity_type'] == 'PER'][['entity_name', 'entity_name_new']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: mapping names of organizations do not work here, only Kamer and WHO makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into persons (PER) and others (ORG, etc.)\n",
    "df_persons = df_exploded[df_exploded['entity_type'] == 'PER']\n",
    "df_others = df_exploded[df_exploded['entity_type'] != 'PER']\n",
    "\n",
    "# Apply the mapping only to persons\n",
    "df_persons_updated = df_persons.groupby('article_id', group_keys=False).apply(map_names_within_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_others if the entity_name is Kamer then change it to Tweede Kamer or if the entity_name is WHO then change it to Wereldgezondheidsorganisatie WHO\n",
    "df_others['entity_name_new'] = df_others['entity_name'].apply(lambda x: 'Tweede Kamer' if x == 'Kamer' else 'Wereldgezondheidsorganisatie WHO' if x == 'WHO' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where entity_name and entity_name_new are different\n",
    "differences_others = df_others[df_others['entity_name'] != df_others['entity_name_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the processed persons back with the rest of the data\n",
    "df_final = pd.concat([df_persons_updated, df_others]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5483, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df_final[df_final.duplicated(subset = ['article_id', 'entity_name_new', 'entity_type'], keep = False)]\n",
    "print(duplicated.shape)\n",
    "\n",
    "# save duplicated to excel for further inspection\n",
    "duplicated.to_excel('coref_resolution/duplicated_entities.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4155, 7)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates. keep first\n",
    "df_final = df_final.drop_duplicates(subset = ['article_id', 'entity_name', 'entity_name_new'], keep = 'first')\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_type\n",
       "1    3704\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.groupby(['article_id', 'entity_name_new'])['entity_type'].nunique().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop entity_name and change entity_name_new to entity_name\n",
    "df_final = df_final.drop(columns = ['entity_name'])\n",
    "df_final.rename(columns = {'entity_name_new': 'entity_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3704, 6)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get the length of the name_variations list\n",
    "df_final['name_variations_length'] = df_final['name_variations'].apply(len)\n",
    "\n",
    "# Sort the DataFrame by article_id, entity_name, and the length of name_variations in descending order\n",
    "df_sorted = df_final.sort_values(by=['article_id', 'entity_name', 'name_variations_length'], ascending=[True, True, False])\n",
    "\n",
    "# Drop duplicates by keeping the first occurrence, which will be the one with the longest name_variations\n",
    "df_unique = df_sorted.drop_duplicates(subset=['article_id', 'entity_name'], keep='first')\n",
    "\n",
    "# Drop the helper column\n",
    "df_unique = df_unique.drop(columns=['name_variations_length'])\n",
    "\n",
    "print(df_unique.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_type\n",
       "1    3704\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_entitytypes = df_unique.groupby(['article_id','entity_name'])['entity_type'].nunique().reset_index()\n",
    "counts_entitytypes.entity_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the df based on article_id, entity_name, and entity_type\n",
    "df_unique = df_unique.sort_values(['article_id', 'entity_name', 'entity_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences Mentioning the Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text, name_variations):\n",
    "    sentences = sent_tokenize(text, language='dutch')\n",
    "    relevant_sentences = [sentence for sentence in sentences if any(name in sentence.lower() for name in name_variations)]\n",
    "    return relevant_sentences\n",
    "\n",
    "df_unique['relevant_sentences'] = df_unique.apply(lambda x: extract_sentences(x['Text'], x['name_variations']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In heel China mogen reisbureaus bovendien per direct geen binnen- of buitenlandse vakanties meer verkopen.',\n",
       " 'Het ministerie van Buitenlandse Zaken zegt dat de ambassade in Peking de situatie op de voet volgt.']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.head()\n",
    "\n",
    "# make exploded_sentences a string by joining the list of sentences\n",
    "df_unique['relevant_sentences_string'] = df_unique['relevant_sentences'].apply(lambda x: ' \\n'.join(x))\n",
    "df_unique.relevant_sentences.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In heel China mogen reisbureaus bovendien per direct geen binnen- of buitenlandse vakanties meer verkopen. \n",
      "Het ministerie van Buitenlandse Zaken zegt dat de ambassade in Peking de situatie op de voet volgt.\n"
     ]
    }
   ],
   "source": [
    "print(df_unique.relevant_sentences_string.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df to csv \n",
    "# df_unique.to_csv('path_to_save.csv', index = False, sep=';', quoting=csv.QUOTE_NONNUMERIC, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Quote Classification DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2666"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit to only articles about covid\n",
    "df_covid = df[df['about_covid'] == 1]\n",
    "len(df_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measures\n",
       "0.0    1590\n",
       "1.0    1076\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create variable measures by taking the max of all measure_ variables\n",
    "df_covid['measures'] = df_covid[['measure_1', 'measure_2', 'measure_3', 'measure_4', 'measure_5',\n",
    "                        'measure_6', 'measure_7', 'measure_8', 'measure_9', 'measure_10',\n",
    "                        'measure_11', 'measure_12', 'measure_13', 'measure_14', 'measure_15',\n",
    "                        'measure_16', 'measure_17']].max(axis = 1)\n",
    "\n",
    "df_covid['measures'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measures_positive\n",
       "0.0    2273\n",
       "1.0     393\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create variable positive measures by taking the max of all measure_ variables\n",
    "df_covid['measures_positive'] = df_covid[['measure_1_positive', 'measure_2_positive', 'measure_3_positive', 'measure_4_positive', 'measure_5_positive',\n",
    "                              'measure_6_positive', 'measure_7_positive', 'measure_8_positive', 'measure_9_positive', 'measure_10_positive',\n",
    "                                'measure_11_positive', 'measure_12_positive', 'measure_13_positive', 'measure_14_positive', 'measure_15_positive',\n",
    "                                'measure_16_positive', 'measure_17_positive']].max(axis = 1)\n",
    "\n",
    "df_covid['measures_positive'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measures_negative\n",
       "0.0    2399\n",
       "1.0     267\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create variable negative measures by taking the max of all measure_ variables\n",
    "df_covid['measures_negative'] = df_covid[['measure_1_negative', 'measure_2_negative', 'measure_3_negative', 'measure_4_negative', 'measure_5_negative',\n",
    "                              'measure_6_negative', 'measure_7_negative', 'measure_8_negative', 'measure_9_negative', 'measure_10_negative',\n",
    "                                'measure_11_negative', 'measure_12_negative', 'measure_13_negative', 'measure_14_negative', 'measure_15_negative',\n",
    "                                'measure_16_negative', 'measure_17_negative']].max(axis = 1)\n",
    "\n",
    "df_covid['measures_negative'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measures_neutral\n",
       "0.0    2107\n",
       "1.0     559\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create variable neutral measures by taking the max of all measure_ variables\n",
    "df_covid['measures_neutral'] = df_covid[['measure_1_neutral', 'measure_2_neutral', 'measure_3_neutral', 'measure_4_neutral', 'measure_5_neutral',\n",
    "                              'measure_6_neutral', 'measure_7_neutral', 'measure_8_neutral', 'measure_9_neutral', 'measure_10_neutral',\n",
    "                                'measure_11_neutral', 'measure_12_neutral', 'measure_13_neutral', 'measure_14_neutral', 'measure_15_neutral',\n",
    "                                'measure_16_neutral', 'measure_17_neutral']].max(axis = 1)\n",
    "\n",
    "df_covid['measures_neutral'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_covid[df_covid['coder'] == 'MainCoder'][['article_id', 'coder', 'actor_name', 'actor_type', 'directly_quoted', 'indirectly_quoted', 'actor_function', 'actor_pp', 'talks_covid_measures', 'measures','measures_positive', 'measures_negative', 'measures_neutral']].drop_duplicates()\n",
    "df_selected['actor_name_normalized'] = df_selected['actor_name'].str.lower()\n",
    "# drop if actor name is nan\n",
    "df_selected = df_selected.dropna(subset = ['actor_name_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name_variations for each actor_name\n",
    "# Create a new column for name variations\n",
    "df_selected['name_variations'] = df_selected['actor_name'].apply(lambda x: generate_name_variations(x))\n",
    "\n",
    "# Drop from name_variations if instance of name_variations matches stopwords\n",
    "df_selected['name_variations'] = df_selected['name_variations'].apply(lambda x: [variation for variation in x if variation not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any duplicates?\n",
    "duplicated = df_selected[df_selected.duplicated(subset = ['article_id', 'actor_name', 'actor_type'], keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_selected = df_unique[['article_id', 'entity_name', 'entity_type', 'relevant_sentences_string', 'name_variations']]\n",
    "df_unique_selected['entity_name_normalized'] = df_unique_selected['entity_name'].str.lower()\n",
    "# drop name variations\n",
    "df_unique_selected.drop(columns = ['name_variations'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 5)\n"
     ]
    }
   ],
   "source": [
    "# see per relevant_sentences_string nr unique entity_names\n",
    "df_unique_selected.groupby('relevant_sentences_string')['entity_name'].nunique().value_counts()\n",
    "\n",
    "# get a check df where relevant_sentences_string has more than one entity_name\n",
    "check_df = df_unique_selected[df_unique_selected['relevant_sentences_string'].duplicated(keep = False)]\n",
    "print(check_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any duplicates?\n",
    "duplicated = df_unique_selected[df_unique_selected.duplicated(subset = ['article_id', 'entity_name', 'entity_type'], keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_type\n",
      "ORG    2425\n",
      "PER    1279\n",
      "Name: count, dtype: int64\n",
      "entity_type\n",
      "Organisatie    2425\n",
      "Persoon        1279\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_unique_selected.entity_type.value_counts())\n",
    "\n",
    "# if entity_type is PER then it is Persoon, if ORG then it is Organisatie\n",
    "df_unique_selected['entity_type'] = np.where(df_unique_selected['entity_type'] == 'PER', 'Persoon', 'Organisatie')\n",
    "print(df_unique_selected.entity_type.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Step 1: Merge on article_id\n",
    "merged_df = pd.merge(df_selected, df_unique_selected, on='article_id', how='inner')\n",
    "\n",
    "# Step 2: Calculate string similarity and keep matches\n",
    "def match_names(row):\n",
    "    # Get actor name and entity name\n",
    "    actor_name = row['actor_name_normalized']\n",
    "    entity_name = row['entity_name_normalized']\n",
    "    # Calculate similarity\n",
    "    similarity = fuzz.token_set_ratio(actor_name, entity_name)\n",
    "    return similarity\n",
    "\n",
    "# Apply the matching function\n",
    "merged_df['similarity'] = merged_df.apply(match_names, axis=1)\n",
    "\n",
    "# Step 3: Filter based on a threshold, e.g., 80, and ensure actor_type matches entity_type\n",
    "threshold = 80\n",
    "final_matches = merged_df[(merged_df['similarity'] >= threshold)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the matches below 90 similarity\n",
    "# final_matches[final_matches['similarity'] < 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 20)\n",
      "(1838, 15)\n",
      "(3704, 5)\n"
     ]
    }
   ],
   "source": [
    "print(final_matches.shape)\n",
    "print(df_selected.shape)\n",
    "print(df_unique_selected.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final_matches to explore\n",
    "final_matches.to_excel('path_to_final_matches.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the actors who are not in the final_matches based on article_id and actor_name\n",
    "len(df_selected[~df_selected['actor_name'].isin(final_matches['actor_name'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatched = df_selected[~df_selected['actor_name'].isin(final_matches['actor_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a column quoted == 1 for final_matches\n",
    "final_matches['quoted'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the df that will be used for the model\n",
    "model_df = final_matches[['article_id', 'entity_name', 'entity_type', 'relevant_sentences_string', 'quoted', 'directly_quoted', 'indirectly_quoted', 'actor_function','actor_pp',\n",
    "                          'talks_covid_measures', 'measures', 'measures_positive', 'measures_negative', 'measures_neutral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entities that are not in final_matches\n",
    "df_unique_selected[~df_unique_selected['entity_name'].isin(final_matches['entity_name'])].head()\n",
    "\n",
    "df_not_quoted = df_unique_selected[~df_unique_selected['entity_name'].isin(final_matches['entity_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_quoted['quoted'] = 0\n",
    "df_not_quoted['directly_quoted'] = 0\n",
    "df_not_quoted['indirectly_quoted'] = 0\n",
    "\n",
    "# select the relevant columns to match with model_df\n",
    "df_not_quoted = df_not_quoted[['article_id', 'entity_name', 'entity_type', 'relevant_sentences_string', 'quoted', 'directly_quoted', 'indirectly_quoted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1124, 7)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_not_quoted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine model_df and df_not_quoted\n",
    "model_df = pd.concat([model_df, df_not_quoted])\n",
    "\n",
    "# create input text by combining entity_name : and relevant_sentences_string\n",
    "model_df['input_text'] = model_df['entity_name'] + ': ' + model_df['relevant_sentences_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df\n",
    "model_df.to_csv('path_to_model_df.csv', index = False, sep=';', quoting=csv.QUOTE_NONNUMERIC, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoted\n",
       "1    1653\n",
       "0    1124\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.quoted.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Model DF After Manual Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.read_csv('path_to_model_df_checked.csv', sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "# make article_id an integer\n",
    "model_df['article_id'] = model_df['article_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoted\n",
       "1.0    1653\n",
       "0.0    1124\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.quoted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoted_check\n",
       "1.0    1681\n",
       "0.0    1096\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.quoted_check.value_counts() # correct column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
