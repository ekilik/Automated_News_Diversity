{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages & Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/\")\n",
    "\n",
    "huggingface_cache_dir = 'model_starling'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [07:02<00:00, 140.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = 'berkeley-nest/Starling-LM-7B-alpha'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=huggingface_cache_dir\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    cache_dir=huggingface_cache_dir\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=huggingface_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "Memory Usage: 3.96816349029541 GB\n",
      "Max Memory Usage: 3.968377113342285 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0,  # 'randomness' of outputs, 0.0 is not possible, so use a very small number\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    article_ids = []\n",
    "    about_covid_values = []\n",
    "\n",
    "    # Define regex pattern for article_id and about_covid with optional double quotes\n",
    "    article_id_pattern = r'\"article_id\"\\s*:\\s*\"?(\\d+)\"?'\n",
    "    about_covid_pattern = r'\"about_covid\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Use regex to find matches for article_id\n",
    "        article_id_match = re.search(article_id_pattern, string_data)\n",
    "\n",
    "        # Use regex to find matches for about_covid\n",
    "        about_covid_match = re.search(about_covid_pattern, string_data)\n",
    "\n",
    "        # Extract values from the regex matches\n",
    "        article_id = int(article_id_match.group(1)) if article_id_match else None\n",
    "        about_covid = int(about_covid_match.group(1)) if about_covid_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        article_ids.append(article_id)\n",
    "        about_covid_values.append(about_covid)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        \"about_covid\": about_covid_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(system_prompt, input_prompt, main_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": input_prompt + \"\\n\" + main_prompt},\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NOS articles annotated by the researcher\n",
    "df = pd.read_csv('NOS/nos_analysis/topic_tests_reliability_data/reliability_topics_elif.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df including text, category, keywords \n",
    "articles_df = pd.read_csv('NOS/nos_analysis/data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n"
     ]
    }
   ],
   "source": [
    "# unique articles from researcher annotations\n",
    "unique_articles = df[['article_id', 'Keywords', 'Category', 'Text']].drop_duplicates()\n",
    "print(unique_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "As a helpful AI assistant, your task is to determine the main topic of news articles. Articles may focus on either the \"Coronavirus and/or the COVID-19 pandemic\" or some other topic.\n",
    "A main topic is the overarching theme discussed in the majority of the news article. For an article to have the main topic of the \"Coronavirus and/or the COVID-19 pandemic\", it should predominantly discuss these subjects.\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = \"\"\"\n",
    "Read the following article with the ID {article_id}: {text} \\n\n",
    "This article falls under the categories: {category} and contains the keywords: {keywords}.\n",
    "\"\"\"\n",
    "\n",
    "main_prompt = \"\"\"\n",
    "Take a moment to understand the article. \n",
    "Remember, for a topic to be a main topic of the news article, it should be discussed in the majority of the article. \n",
    "\n",
    "Based on the information provided, determine if the main topic of this news article is the \"Coronavirus and/or the COVID-19 pandemic\" or another subject. \n",
    "Assign a value of 1 if the main topic is the \"Coronavirus and/or the COVID-19 pandemic\", and a value of 0 if it is another subject.\n",
    "\n",
    "Output your results in JSON format with keys \"article_id\" and \"about_covid\", where the article ID and your answer are the values. \n",
    "Follow the example output format provided. Do not include any additional information or explanation. \n",
    "\n",
    "Example Output (JSON format):\n",
    "{{\n",
    "    \"article_id\": \"2000000\",\n",
    "    \"about_covid\": \"1\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating Articles About Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>GPT4 Correct System: \n",
      "As a helpful AI assistant, your task is to determine the main topic of news articles. Articles may focus on either the \"Coronavirus and/or the COVID-19 pandemic\" or some other topic.\n",
      "A main topic is the overarching theme discussed in the majority of the news article. For an article to have the main topic of the \"Coronavirus and/or the COVID-19 pandemic\", it should predominantly discuss these subjects.\n",
      "<|end_of_turn|>GPT4 Correct User: \n",
      "Read the following article with the ID {article_id}: {text} \n",
      "\n",
      "This article falls under the categories: {category} and contains the keywords: {keywords}.\n",
      "\n",
      "\n",
      "Take a moment to understand the article. \n",
      "Remember, for a topic to be a main topic of the news article, it should be discussed in the majority of the article. \n",
      "\n",
      "Based on the information provided, determine if the main topic of this news article is the \"Coronavirus and/or the COVID-19 pandemic\" or another subject. \n",
      "Assign a value of 1 if the main topic is the \"Coronavirus and/or the COVID-19 pandemic\", and a value of 0 if it is another subject.\n",
      "\n",
      "Output your results in JSON format with keys \"article_id\" and \"about_covid\", where the article ID and your answer are the values. \n",
      "Follow the example output format provided. Do not include any additional information or explanation. \n",
      "\n",
      "Example Output (JSON format):\n",
      "{{\n",
      "    \"article_id\": \"2000000\",\n",
      "    \"about_covid\": \"1\"\n",
      "}}\n",
      "<|end_of_turn|>GPT4 Correct Assistant:\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = zero_shot_prompt_messages(system_prompt, input_prompt, main_prompt)\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"text\", \"category\", \"keywords\"],\n",
    "    template=zero_shot_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'category', 'keywords', 'text'], template='<s>GPT4 Correct System: \\nAs a helpful AI assistant, your task is to determine the main topic of news articles. Articles may focus on either the \"Coronavirus and/or the COVID-19 pandemic\" or some other topic.\\nA main topic is the overarching theme discussed in the majority of the news article. For an article to have the main topic of the \"Coronavirus and/or the COVID-19 pandemic\", it should predominantly discuss these subjects.\\n<|end_of_turn|>GPT4 Correct User: \\nRead the following article with the ID {article_id}: {text} \\n\\nThis article falls under the categories: {category} and contains the keywords: {keywords}.\\n\\n\\nTake a moment to understand the article. \\nRemember, for a topic to be a main topic of the news article, it should be discussed in the majority of the article. \\n\\nBased on the information provided, determine if the main topic of this news article is the \"Coronavirus and/or the COVID-19 pandemic\" or another subject. \\nAssign a value of 1 if the main topic is the \"Coronavirus and/or the COVID-19 pandemic\", and a value of 0 if it is another subject.\\n\\nOutput your results in JSON format with keys \"article_id\" and \"about_covid\", where the article ID and your answer are the values. \\nFollow the example output format provided. Do not include any additional information or explanation. \\n\\nExample Output (JSON format):\\n{{\\n    \"article_id\": \"2000000\",\\n    \"about_covid\": \"1\"\\n}}\\n<|end_of_turn|>GPT4 Correct Assistant:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7080fd0be040>), output_key='article_id, about_covid')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_one = LLMChain(llm = llm, prompt = prompt_template, output_key=\"article_id, about_covid\")\n",
    "chain_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for index, row in unique_articles.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    text = row['Text']\n",
    "    category = row['Category']\n",
    "    keywords = row['Keywords']\n",
    "\n",
    "\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"text\": text,\n",
    "            \"category\": category,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_one.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot.append(generated_text)    \n",
    "\n",
    "# CPU times: user 3min 48s, sys: 14.8 s, total: 4min 3s\n",
    "# Wall time: 4min 10s\n",
    "# output cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = regex_extract_to_dataframe(generated_text_zeroshot)\n",
    "df_zeroshot = pd.DataFrame(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>about_covid_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2322751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2325123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2326923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2328119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2328418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  about_covid_pred\n",
       "0     2322751                 1\n",
       "1     2325123                 1\n",
       "2     2326923                 1\n",
       "3     2328119                 1\n",
       "4     2328418                 1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename about_covid to about_covid_pred\n",
    "df_zeroshot = df_zeroshot.rename(columns={'about_covid': 'about_covid_pred'})\n",
    "df_zeroshot['article_id'] = df_zeroshot['article_id'].astype(int)\n",
    "df_zeroshot['about_covid_pred'] = df_zeroshot['about_covid_pred'].astype(int)\n",
    "\n",
    "df_zeroshot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elif = df[df['coder'] == 'Elif Kilik']\n",
    "df_coded_elif = df_elif[['article_id', 'about_covid']]\n",
    "df_zeroshot_merged_elif = pd.merge(df_coded_elif, df_zeroshot, how='left', on=\"article_id\")\n",
    "print(len(df_zeroshot_merged_elif))\n",
    "\n",
    "df_zeroshot_merged_elif = df_zeroshot_merged_elif.dropna()\n",
    "df_zeroshot_merged_elif['about_covid_pred'] = df_zeroshot_merged_elif['about_covid_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        42\n",
      "           1       0.96      0.96      0.96        78\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.95      0.95      0.95       120\n",
      "weighted avg       0.95      0.95      0.95       120\n",
      "\n",
      "Cohen's Kappa: 0.8901098901098901\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_zeroshot_merged_elif['about_covid'], df_zeroshot_merged_elif['about_covid_pred']))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(df_zeroshot_merged_elif['about_covid'], df_zeroshot_merged_elif['about_covid_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
