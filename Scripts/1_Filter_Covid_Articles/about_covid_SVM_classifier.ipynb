{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import  classification_report, cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from transformers import set_seed\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/actor_classification\")\n",
    "\n",
    "\n",
    "huggingface_cache_dir = 'model'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/eklk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the manually annotated dataset\n",
    "df = pd.read_csv('data/coded_df_topics_full.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df includes all article texts, categories and keywords\n",
    "articles_df = pd.read_csv('data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Title', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)\n",
    "# change LINE BREAK to \\n\n",
    "articles_df['Text'] = articles_df['Text'].str.replace('[LINE_BREAK]', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an input text, combining Title, Text, Category and Keywords and before Category add string 'Categories: ' and before Keywords add string 'Keywords: ' if Category and Keywords are empty skip them\n",
    "articles_df['Category'] = articles_df['Category'].fillna('')\n",
    "articles_df['Keywords'] = articles_df['Keywords'].fillna('')\n",
    "articles_df['input_text'] = articles_df['Title'] + '\\n' + articles_df['Text'] + '\\n' + 'Categories: ' + articles_df['Category'] + ' ' + 'Keywords: ' + articles_df['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minister roept iedereen op: niet discrimineren om coronavirus \n",
      "Minister Bruins voor Medische Zorg vindt het verschrikkelijk dat mensen met een Aziatisch uiterlijk worden gediscrimineerd vanwege het coronavirus. Hij deed in de Tweede Kamer een oproep aan iedereen om hiertegen op te staan.\n",
      "Bruins reageerde op vragen van onder anderen GroenLinks-Kamerlid Ellemeet. Zij zei dat mensen met Aziatisch uiterlijk op grote schaal worden gediscrimineerd. Ze hoorde bijvoorbeeld van een meisje dat mensen in de tram hun trui over hun mond trokken toen ze haar zagen.\n",
      "Ellemeet vroeg de minister of hij zich hierover duidelijk wil uitspreken. Bruins zei hierop dat hij dit de komende dagen nog verschillende keren wil doen, ook buiten de Tweede Kamer.\n",
      "De minister zei dat hij het er zeer mee eens is dat dit niet bij een fatsoenlijke samenleving hoort. \"Mensen discrimineren gaat niet aan. We moeten ervoor zorgen dat het niet optreedt. Daar hebben wij allemaal een rol in.\"\n",
      "Bruins is niet van plan om evenementen en attracties te sluiten die veel Chinese toeristen trekken, zoals de Keukenhof. De PVV wilde weten of dit verstandig zou zijn. De minister vindt het overbodig, omdat het risico dat mensen in de Keukenhof worden besmet heel klein is. Bovendien werkt zo'n maatregel discriminerend en stigmatiserend, zei hij.\n",
      "Categories: Politiek Keywords: Bruno Bruins, coronavirus, Corinne Ellemeet\n",
      "minister roept iedereen op: niet discrimineren om coronavirus \n",
      "minister bruins voor medische zorg vindt het verschrikkelijk dat mensen met een aziatisch uiterlijk worden gediscrimineerd vanwege het coronavirus. hij deed in de tweede kamer een oproep aan iedereen om hiertegen op te staan.\n",
      "bruins reageerde op vragen van onder anderen groenlinks-kamerlid ellemeet. zij zei dat mensen met aziatisch uiterlijk op grote schaal worden gediscrimineerd. ze hoorde bijvoorbeeld van een meisje dat mensen in de tram hun trui over hun mond trokken toen ze haar zagen.\n",
      "ellemeet vroeg de minister of hij zich hierover duidelijk wil uitspreken. bruins zei hierop dat hij dit de komende dagen nog verschillende keren wil doen, ook buiten de tweede kamer.\n",
      "de minister zei dat hij het er zeer mee eens is dat dit niet bij een fatsoenlijke samenleving hoort. \"mensen discrimineren gaat niet aan. we moeten ervoor zorgen dat het niet optreedt. daar hebben wij allemaal een rol in.\"\n",
      "bruins is niet van plan om evenementen en attracties te sluiten die veel chinese toeristen trekken, zoals de keukenhof. de pvv wilde weten of dit verstandig zou zijn. de minister vindt het overbodig, omdat het risico dat mensen in de keukenhof worden besmet heel klein is. bovendien werkt zo'n maatregel discriminerend en stigmatiserend, zei hij.\n",
      "categories: politiek keywords: bruno bruins, coronavirus, corinne ellemeet\n"
     ]
    }
   ],
   "source": [
    "print(df['input_text'].values[0])\n",
    "\n",
    "# create a text preprocessing function where you lowercase the text and then lemmitize the text\n",
    "def text_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['input_text_lower'] = df['input_text'].apply(text_lower)\n",
    "print(df['input_text_lower'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minister oproepen iedereen op : niet discrimineren om coronavirus \n",
      " minister bruins voor medisch zorg vinden het verschrikkelijk dat mens met een aziatisch uiterlijk worden discrimineren vanwege het coronavirus . hij doen in de Tweede Kamer een oproep aan iedereen om hiertegen op te bijstaan . \n",
      " bruins reageren op vraag van onder ander groenlinks-kamerlid ellemeet . zij zeggen dat mens met aziatisch uiterlijk op groot schaal worden discrimineren . ze horen bijvoorbeeld van een meisje dat mens in de tram hun trui over hun mond trekken toen ze haar zien . \n",
      " ellemeet vragen de minister of hij zich hierover duidelijk willen uitspreken . bruins zeggen hierop dat hij dit de komen dag nog verschillend keer willen doen , ook buiten de Tweede Kamer . \n",
      " de minister zeggen dat hij het er zeer mee eens zijn dat dit niet bij een fatsoenlijk samenleving horen . \" mens discrimineren gaan niet aan . we moeten ervoor zorgen dat het niet optreden . daar hebben wij allemaal een rol in . \" \n",
      " bruins zijn niet van plan om evenement en attractie te sluiten die veel chinees toerist trekken , zoals de Keukenhof . de pvv willen weten of dit verstandig zullen zijn . de minister vinden het overbodig , omdat het risico dat mens in de Keukenhof worden besmet heel klein zijn . bovendien werken zo'n maatregel discrimineren en stigmatiseren , zeggen hij . \n",
      " categorie : politiek keywords : Bruno bruins , coronavirus , Corinne ellemeet\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "def text_lemmatizer(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([token.lemma_ for token in text])\n",
    "    return text\n",
    "\n",
    "df['input_text_lemmatized'] = df['input_text_lower'].apply(text_lemmatizer)\n",
    "print(df['input_text_lemmatized'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['reliability_article'] == 0]\n",
    "test_df = df[df['reliability_article'] == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (540, 1)\n",
      "y_train shape: (540,)\n"
     ]
    }
   ],
   "source": [
    "# Select your features and target variable\n",
    "X = train_df[['input_text_lower']]  # This should remain a DataFrame\n",
    "y = train_df['about_covid'].values.flatten()  # Convert to 1D array\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")  # Should be (n_samples_train, n_features)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # Should be (n_samples_train,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid_svc = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__max_features': [100, 2000, 1000, 5000, 10000],\n",
    "    'clf__C': [0.1, 1, 10, 50, 100],\n",
    "    'clf__max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "# Define the SVC pipeline\n",
    "pipeline_svc = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1), analyzer='word', stop_words=stopwords)),\n",
    "    ('clf', LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# Perform grid search for SVC\n",
    "grid_search_svc = GridSearchCV(pipeline_svc, param_grid_svc, cv=10, scoring='f1_macro')\n",
    "grid_search_svc.fit(X_train['input_text_lower'], y_train)  # Use the column name directly\n",
    "\n",
    "print(f\"Best parameters for SVC: {grid_search_svc.best_params_}\")\n",
    "print(f\"Best score for SVC: {grid_search_svc.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found for SVC:  {'clf__C': 1, 'clf__max_iter': 100, 'tfidf__max_features': 1000, 'tfidf__ngram_range': (1, 2)}\n",
      "Best model found for SVC:  Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2),\n",
      "                                 stop_words=['de', 'en', 'van', 'ik', 'te',\n",
      "                                             'dat', 'die', 'in', 'een', 'hij',\n",
      "                                             'het', 'niet', 'zijn', 'is', 'was',\n",
      "                                             'op', 'aan', 'met', 'als', 'voor',\n",
      "                                             'had', 'er', 'maar', 'om', 'hem',\n",
      "                                             'dan', 'zou', 'of', 'wat', 'mijn', ...])),\n",
      "                ('clf', LinearSVC(C=1, max_iter=100, random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters and the best model\n",
    "best_params = grid_search_svc.best_params_\n",
    "best_model = grid_search_svc.best_estimator_\n",
    "\n",
    "print(\"Best parameters found for SVC: \", best_params)\n",
    "print(\"Best model found for SVC: \", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions for the validation set\n",
    "val_preds = best_model.predict(X_val['input_text_lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "Actual           \n",
       "0          46  11\n",
       "1          10  69"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(val_labels, val_preds, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81        56\n",
      "           1       0.87      0.86      0.87        80\n",
      "\n",
      "    accuracy                           0.85       136\n",
      "   macro avg       0.84      0.84      0.84       136\n",
      "weighted avg       0.85      0.85      0.85       136\n",
      "\n",
      "cohen kappa\n",
      "0.6821015138023152\n"
     ]
    }
   ],
   "source": [
    "print('classification report')\n",
    "print(classification_report(val_preds, val_labels))\n",
    "print('cohen kappa')\n",
    "print(cohen_kappa_score(val_preds, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the reliability df\n",
    "reliability_df = pd.read_csv('data/reliability_topics_elif.csv',\n",
    "                             sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "reliability_df['article_id'] = reliability_df['article_id'].astype(int)\n",
    "\n",
    "reliability_df.head()\n",
    "\n",
    "# merge reliability_df with articles_df\n",
    "reliability_df = pd.merge(reliability_df, articles_df, on='article_id', how = 'left')\n",
    "print(reliability_df.shape)\n",
    "\n",
    "# make an input text, combining Title, Text, Category and Keywords and before Category add string 'Categories: ' and before Keywords add string 'Keywords: ' if Category and Keywords are empty skip them\n",
    "reliability_df['Category'] = reliability_df['Category'].fillna('')\n",
    "reliability_df['Keywords'] = reliability_df['Keywords'].fillna('')\n",
    "reliability_df['input_text'] = reliability_df['Text'] + '\\n' + 'Categories: ' + reliability_df['Category'] + ' ' + 'Keywords: ' + reliability_df['Keywords']\n",
    "reliability_df['input_text_lower'] = reliability_df['input_text'].apply(text_lower)\n",
    "\n",
    "# make a test_df for all coders separately\n",
    "test_df_elif = reliability_df[reliability_df['coder'] == 'Elif Kilik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about_covid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0         0   1\n",
       "about_covid        \n",
       "0.0          35   7\n",
       "1.0          12  66"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the predictions for the test set\n",
    "test_preds_elif = best_model.predict(test_df_elif['input_text_lower'])\n",
    "\n",
    "pd.crosstab(test_df_elif['about_covid'], test_preds_elif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.74      0.79        47\n",
      "           1       0.85      0.90      0.87        73\n",
      "\n",
      "    accuracy                           0.84       120\n",
      "   macro avg       0.84      0.82      0.83       120\n",
      "weighted avg       0.84      0.84      0.84       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report \n",
    "print('classification report')\n",
    "print(classification_report(test_preds_elif, test_df_elif['about_covid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/svm_about_covid.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the best model for svm\n",
    "joblib.dump(best_model, 'model/svm_about_covid.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistralenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
