{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = 'berkeley-nest/Starling-LM-7B-alpha'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 3090\n",
      "Memory Usage: 3.84267520904541 GB\n",
      "Max Memory Usage: 3.9352827072143555 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0,  # 'randomness' of outputs, 0.0 is not possible, so use a very small number\n",
    "    max_new_tokens=3000,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(system_prompt, main_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": main_prompt},\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def count_words(sentences_list):\n",
    "    # Join the list into a single string\n",
    "    if sentences_list not in [None, np.nan]:\n",
    "        full_text = ' '.join(sentences_list)\n",
    "        # Split the string into words and count them\n",
    "        word_count = len(full_text.split())\n",
    "    else:\n",
    "        word_count = 0\n",
    "    return word_count\n",
    "\n",
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    source_names = []\n",
    "    measures_mentioned_values = []\n",
    "    supportive_stance_values = []\n",
    "    critical_stance_values = []\n",
    "\n",
    "    # Define regex pattern for structured output\n",
    "    assistant_output_pattern = r'<\\|end_of_turn\\|>GPT4 Correct Assistant:\\s*({.*})'\n",
    "    \n",
    "    # Define regex patterns for JSON fields\n",
    "    source_name_pattern = r'\"source_name\"\\s*:\\s*\"?([^\"]+)\"?'\n",
    "    measures_mentioned_pattern = r'\"measures_mentioned\"\\s*:\\s*\"?(\\d)\"?'\n",
    "    supportive_stance_pattern = r'\"supportive_stance\"\\s*:\\s*\"?(\\d)\"?'\n",
    "    critical_stance_pattern = r'\"critical_stance\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Extract content after \"<|end_of_turn|>GPT4 Correct Assistant:\"\n",
    "        match = re.search(assistant_output_pattern, string_data, re.DOTALL)\n",
    "        if match:\n",
    "            json_data = match.group(1).replace(\"\\\\\", \"\")  # Extract and clean JSON part\n",
    "        else:\n",
    "            continue  # Skip this string if no match found\n",
    "\n",
    "        # Use regex to find matches for each field\n",
    "        source_name_match = re.search(source_name_pattern, json_data)\n",
    "        measures_mentioned_match = re.search(measures_mentioned_pattern, json_data)\n",
    "        supportive_stance_match = re.search(supportive_stance_pattern, json_data)\n",
    "        critical_stance_match = re.search(critical_stance_pattern, json_data)\n",
    "\n",
    "        # Extract values from regex matches\n",
    "        source_name = source_name_match.group(1) if source_name_match else None\n",
    "        measures_mentioned = int(measures_mentioned_match.group(1)) if measures_mentioned_match else None\n",
    "        supportive_stance = int(supportive_stance_match.group(1)) if supportive_stance_match else None\n",
    "        critical_stance = int(critical_stance_match.group(1)) if critical_stance_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        source_names.append(source_name)\n",
    "        measures_mentioned_values.append(measures_mentioned)\n",
    "        supportive_stance_values.append(supportive_stance)\n",
    "        critical_stance_values.append(critical_stance)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"source_name\": source_names,\n",
    "        \"measures_mentioned\": measures_mentioned_values,\n",
    "        \"supportive_stance\": supportive_stance_values,\n",
    "        \"critical_stance\": critical_stance_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Actor DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('path_to_test_data_researcher_codings',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "# keep only if directly_quoted or indirectly_quoted is 1\n",
    "df = df[(df['directly_quoted'] == 1) | (df['indirectly_quoted'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article_id, actor_name, talks_covid_measures, relevant_sentences_string\n",
    "df = df[['article_id', 'actor_name', 'talks_covid_measures', 'input_text_corrected',\n",
    "       'talks_covid_corrected', 'measures_positive_corrected',\n",
    "       'measures_negative_corrected', 'measures_neutral_corrected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rename actor_name to source_name\n",
    "df_selected = df.rename(columns={'actor_name': 'source_name'})\n",
    "\n",
    "# sort by article_id, actor_name\n",
    "df_selected = df_selected.sort_values(by=['article_id', 'source_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the Actor Talk about Covid Measures? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful AI assistant. Sources in a news article are people or organizations that are quoted in the article. \n",
    "You will be provided with the names of these sources and the sentences from the news article where these sources are mentioned. Your task is to determine if the source discusses any COVID-19 measures and, if so, assess the source's overall stance towards these measures.\n",
    "\"\"\"\n",
    "\n",
    "main_prompt = \"\"\"\n",
    "Some examples of COVID-19 Measures:\n",
    "1. General Coronavirus Measures: Broad discussions about pandemic measures and advice without specifying details.\n",
    "2. Curfew, Lockdown, Quarantine Rules: References to staying home, lockdowns, quarantines, and related rules.\n",
    "3. COVID-19 Testing: Discussions about testing requirements and access cards or travel passes related to testing (e.g., Testing for Access (1G)).\n",
    "4. Vaccination and Procedures: Discussions about vaccines, vaccination processes, side effects, and access cards or travel passes related to vaccines (e.g., 2G or 3G passes).\n",
    "5. Measures about Sports, Events and Gatherings: Regulations about events, sports, gatherings, and social distancing.\n",
    "6. Closing of Schools, Restaurants, Shops: References to closures of schools, restaurants, shops, or other public places.\n",
    "7. Other advice and Recommendations: Wearing face masks, working from home, social distancing, limiting visits, and other health advice.\n",
    "\n",
    "Read the extracted information from the news article with ID {article_id} about the source: {source_name}. The following text begins with the source name and provides the sentences the source is mentioned in the article: {input_text_corrected}.\n",
    "\n",
    "1. Carefully review the information provided.\n",
    "\n",
    "2. Determine if the source mentions any COVID-19 measures:\n",
    "   - If yes, set measures_mentioned to 1.\n",
    "   - If no, set measures_mentioned to 0.\n",
    "\n",
    "3. If measures_mentioned is 1, classify the source's stance towards the COVID-19 measures with two binary flags. \n",
    "   - Supportive stance: Set supportive_stance to 1 if the source is positive about the measures, expresses general support or approval towards measures, or generally agrees with the existance of measures.\n",
    "   - Critical stance: Set critical_stance to 1 the source is negative about the measures,expresses general opposition or criticism towards measures, discusses negative aspects, or disagrees with the existance of  measures.\n",
    "   - Set supportive_stance and critical_stance to 0 if the source does not mention COVID-19 measures at all or does not express a supportive or critical stance.\n",
    "   - Keep in mind that the source may express both supportive and critical stances, so both flags can be 1.\n",
    "\n",
    "4. Provide the result in the following JSON format. Do not include any additional information or explanation.\n",
    "\n",
    "### Example Output (JSON format):\n",
    "{{\n",
    "  \"article_id\": \"2000000\",\n",
    "  \"source_name\": \"Herman Kroneman\",\n",
    "  \"measures_mentioned\": 1,\n",
    "  \"supportive_stance\": 1,\n",
    "  \"critical_stance\": 0\n",
    "}}\n",
    "{{\n",
    "  \"article_id\": \"2000001\",\n",
    "  \"source_name\": \"De Volkskrant\",\n",
    "  \"measures_mentioned\": 0,\n",
    "  \"supportive_stance\": 0,\n",
    "  \"critical_stance\": 0\n",
    "}}\n",
    "{{\n",
    "  \"article_id\": \"2000002\",\n",
    "  \"source_name\": \"RIVM\",\n",
    "  \"measures_mentioned\": 1,\n",
    "  \"supportive_stance\": 0,\n",
    "  \"critical_stance\": 1\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>GPT4 Correct System: \n",
      "You are a helpful AI assistant. Sources in a news article are people or organizations that are quoted in the article. \n",
      "You will be provided with the names of these sources and the sentences from the news article where these sources are mentioned. Your task is to determine if the source discusses any COVID-19 measures and, if so, assess the source's overall stance towards these measures.\n",
      "<|end_of_turn|>GPT4 Correct User: \n",
      "Some examples of COVID-19 Measures:\n",
      "1. General Coronavirus Measures: Broad discussions about pandemic measures and advice without specifying details.\n",
      "2. Curfew, Lockdown, Quarantine Rules: References to staying home, lockdowns, quarantines, and related rules.\n",
      "3. COVID-19 Testing: Discussions about testing requirements and access cards or travel passes related to testing (e.g., Testing for Access (1G)).\n",
      "4. Vaccination and Procedures: Discussions about vaccines, vaccination processes, side effects, and access cards or travel passes related to vaccines (e.g., 2G or 3G passes).\n",
      "5. Measures about Sports, Events and Gatherings: Regulations about events, sports, gatherings, and social distancing.\n",
      "6. Closing of Schools, Restaurants, Shops: References to closures of schools, restaurants, shops, or other public places.\n",
      "7. Other advice and Recommendations: Wearing face masks, working from home, social distancing, limiting visits, and other health advice.\n",
      "\n",
      "Read the extracted information from the news article with ID {article_id} about the source: {source_name}. The following text begins with the source name and provides the sentences the source is mentioned in the article: {input_text_corrected}.\n",
      "\n",
      "1. Carefully review the information provided.\n",
      "\n",
      "2. Determine if the source mentions any COVID-19 measures:\n",
      "   - If yes, set measures_mentioned to 1.\n",
      "   - If no, set measures_mentioned to 0.\n",
      "\n",
      "3. If measures_mentioned is 1, classify the source's stance towards the COVID-19 measures with two binary flags. \n",
      "   - Supportive stance: Set supportive_stance to 1 if the source iS positive about the measures, expresses general support or approval towards measures, or generally agrees with the existance of measures.\n",
      "   - Critical stance: Set critical_stance to 1 the source is negative about the measures,expresses general opposition or criticism towards measures, discusses negative aspects, or disagrees with the existance of  measures.\n",
      "   - Set supportive_stance and critical_stance to 0 if the source does not mention COVID-19 measures at all or does not express a supportive or critical stance.\n",
      "   - Keep in mind that the source may express both supportive and critical stances, so both flags can be 1.\n",
      "\n",
      "4. Provide the result in the following JSON format. Do not include any additional information or explanation.\n",
      "\n",
      "### Example Output (JSON format):\n",
      "{{\n",
      "  \"article_id\": \"2000000\",\n",
      "  \"source_name\": \"Herman Kroneman\",\n",
      "  \"measures_mentioned\": 1,\n",
      "  \"supportive_stance\": 1,\n",
      "  \"critical_stance\": 0\n",
      "}}\n",
      "{{\n",
      "  \"article_id\": \"2000001\",\n",
      "  \"source_name\": \"De Volkskrant\",\n",
      "  \"measures_mentioned\": 0,\n",
      "  \"supportive_stance\": 0,\n",
      "  \"critical_stance\": 0\n",
      "}}\n",
      "{{\n",
      "  \"article_id\": \"2000002\",\n",
      "  \"source_name\": \"RIVM\",\n",
      "  \"measures_mentioned\": 1,\n",
      "  \"supportive_stance\": 0,\n",
      "  \"critical_stance\": 1\n",
      "}}\n",
      "<|end_of_turn|>GPT4 Correct Assistant:\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = zero_shot_prompt_messages(system_prompt, main_prompt)\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"source_name\", \"input_text_corrected\"],\n",
    "    template=zero_shot_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'input_text_corrected', 'source_name'], template='<s>GPT4 Correct System: \\nYou are a helpful AI assistant. Sources in a news article are people or organizations that are quoted in the article. \\nYou will be provided with the names of these sources and the sentences from the news article where these sources are mentioned. Your task is to determine if the source discusses any COVID-19 measures and, if so, assess the source\\'s overall stance towards these measures.\\n<|end_of_turn|>GPT4 Correct User: \\nSome examples of COVID-19 Measures:\\n1. General Coronavirus Measures: Broad discussions about pandemic measures and advice without specifying details.\\n2. Curfew, Lockdown, Quarantine Rules: References to staying home, lockdowns, quarantines, and related rules.\\n3. COVID-19 Testing: Discussions about testing requirements and access cards or travel passes related to testing (e.g., Testing for Access (1G)).\\n4. Vaccination and Procedures: Discussions about vaccines, vaccination processes, side effects, and access cards or travel passes related to vaccines (e.g., 2G or 3G passes).\\n5. Measures about Sports, Events and Gatherings: Regulations about events, sports, gatherings, and social distancing.\\n6. Closing of Schools, Restaurants, Shops: References to closures of schools, restaurants, shops, or other public places.\\n7. Other advice and Recommendations: Wearing face masks, working from home, social distancing, limiting visits, and other health advice.\\n\\nRead the extracted information from the news article with ID {article_id} about the source: {source_name}. The following text begins with the source name and provides the sentences the source is mentioned in the article: {input_text_corrected}.\\n\\n1. Carefully review the information provided.\\n\\n2. Determine if the source mentions any COVID-19 measures:\\n   - If yes, set measures_mentioned to 1.\\n   - If no, set measures_mentioned to 0.\\n\\n3. If measures_mentioned is 1, classify the source\\'s stance towards the COVID-19 measures with two binary flags. \\n   - Supportive stance: Set supportive_stance to 1 if the source iS positive about the measures, expresses general support or approval towards measures, or generally agrees with the existance of measures.\\n   - Critical stance: Set critical_stance to 1 the source is negative about the measures,expresses general opposition or criticism towards measures, discusses negative aspects, or disagrees with the existance of  measures.\\n   - Set supportive_stance and critical_stance to 0 if the source does not mention COVID-19 measures at all or does not express a supportive or critical stance.\\n   - Keep in mind that the source may express both supportive and critical stances, so both flags can be 1.\\n\\n4. Provide the result in the following JSON format. Do not include any additional information or explanation.\\n\\n### Example Output (JSON format):\\n{{\\n  \"article_id\": \"2000000\",\\n  \"source_name\": \"Herman Kroneman\",\\n  \"measures_mentioned\": 1,\\n  \"supportive_stance\": 1,\\n  \"critical_stance\": 0\\n}}\\n{{\\n  \"article_id\": \"2000001\",\\n  \"source_name\": \"De Volkskrant\",\\n  \"measures_mentioned\": 0,\\n  \"supportive_stance\": 0,\\n  \"critical_stance\": 0\\n}}\\n{{\\n  \"article_id\": \"2000002\",\\n  \"source_name\": \"RIVM\",\\n  \"measures_mentioned\": 1,\\n  \"supportive_stance\": 0,\\n  \"critical_stance\": 1\\n}}\\n<|end_of_turn|>GPT4 Correct Assistant:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x0000019F62B76B20>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_one = LLMChain(llm = llm, prompt = prompt_template)\n",
    "chain_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot_actors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "for index, row in df_selected.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    source_name = row['source_name']\n",
    "    input_text_corrected = row['input_text_corrected']\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"source_name\": source_name,\n",
    "            \"input_text_corrected\": input_text_corrected}\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_one.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot_actors.append(generated_text)    \n",
    "\n",
    "# CPU times: total: 15min 37s\n",
    "# Wall time: 16min 38s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated = regex_extract_to_dataframe(generated_text_zeroshot_actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measures_mentioned\n",
       "1    166\n",
       "0    129\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generated.measures_mentioned.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supportive_stance\n",
       "0    272\n",
       "1     23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generated.supportive_stance.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "critical_stance\n",
       "0    250\n",
       "1     45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generated.critical_stance.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the generated data with the original data on index\n",
    "df_merged = df_selected.merge(df_generated, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mentions_covid_measures to the original dataframe\n",
    "df_selected['measures_mentioned'] = df_generated['measures_mentioned']\n",
    "df_selected['supportive_stance'] = df_generated['supportive_stance']\n",
    "df_selected['critical_stance'] = df_generated['critical_stance']\n",
    "df_selected['actor_name_starling'] = df_selected['source_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>talks_covid_measures</th>\n",
       "      <th>input_text_corrected</th>\n",
       "      <th>talks_covid_corrected</th>\n",
       "      <th>measures_positive_corrected</th>\n",
       "      <th>measures_negative_corrected</th>\n",
       "      <th>measures_neutral_corrected</th>\n",
       "      <th>measures_mentioned</th>\n",
       "      <th>supportive_stance</th>\n",
       "      <th>critical_stance</th>\n",
       "      <th>actor_name_starling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_id, source_name, talks_covid_measures, input_text_corrected, talks_covid_corrected, measures_positive_corrected, measures_negative_corrected, measures_neutral_corrected, measures_mentioned, supportive_stance, critical_stance, actor_name_starling]\n",
       "Index: []"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see where source_name and actor_name are different\n",
    "df_selected[df_selected['source_name'] != df_selected['actor_name_starling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>measures_mentioned</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talks_covid_measures</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>82</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>47</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "measures_mentioned     0    1\n",
       "talks_covid_measures         \n",
       "0.0                   82   64\n",
       "1.0                   47  102"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crosstab talks_covid_measures and mentions_covid_measures\n",
    "pd.crosstab(df_selected['talks_covid_measures'], df_selected['measures_mentioned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>supportive_stance</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measures_positive_corrected</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>247</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "supportive_stance              0   1\n",
       "measures_positive_corrected         \n",
       "0.0                          247   8\n",
       "1.0                           25  15"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_selected['measures_positive_corrected'], df_selected['supportive_stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>critical_stance</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measures_negative_corrected</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>234</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "critical_stance                0   1\n",
       "measures_negative_corrected         \n",
       "0.0                          234  32\n",
       "1.0                           16  13"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_selected['measures_negative_corrected'], df_selected['critical_stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.56      0.60       146\n",
      "         1.0       0.61      0.68      0.65       149\n",
      "\n",
      "    accuracy                           0.62       295\n",
      "   macro avg       0.63      0.62      0.62       295\n",
      "weighted avg       0.62      0.62      0.62       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get classification report\n",
    "print(classification_report(df_selected['talks_covid_measures'], df_selected['measures_mentioned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks_covid = df_selected[df_selected['talks_covid_measures'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.96      0.90       113\n",
      "         1.0       0.79      0.42      0.55        36\n",
      "\n",
      "    accuracy                           0.83       149\n",
      "   macro avg       0.81      0.69      0.72       149\n",
      "weighted avg       0.83      0.83      0.81       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(talks_covid['measures_positive_corrected'], talks_covid['supportive_stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.86      0.87       121\n",
      "         1.0       0.43      0.46      0.45        28\n",
      "\n",
      "    accuracy                           0.79       149\n",
      "   macro avg       0.65      0.66      0.66       149\n",
      "weighted avg       0.79      0.79      0.79       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(talks_covid['measures_negative_corrected'], talks_covid['critical_stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "df_selected.to_csv('path_to_save_test_df_with_predictions/stance_Starling.csv',\n",
    "          sep = ';', encoding = 'utf-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
