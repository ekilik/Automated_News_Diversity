{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74581/2950570917.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/home/eklk/.conda/envs/mistralenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, set_seed\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/actor_classification\")\n",
    "\n",
    "\n",
    "huggingface_cache_dir = 'model'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/coded_df_topics_full.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(df.shape)\n",
    "\n",
    "df = df[df['about_covid'] == 1]\n",
    "print(df.shape)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df\n",
    "articles_df = pd.read_csv('data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Title', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)\n",
    "# change LINE BREAK to \\n\n",
    "articles_df['Text'] = articles_df['Text'].str.replace('[LINE_BREAK]', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an input text, combining Title, Text, Category and Keywords and before Category add string 'Categories: ' and before Keywords add string 'Keywords: ' if Category and Keywords are empty skip them\n",
    "articles_df['Category'] = articles_df['Category'].fillna('')\n",
    "articles_df['Keywords'] = articles_df['Keywords'].fillna('')\n",
    "articles_df['input_text'] = articles_df['Title'] + '\\n' + articles_df['Text'] + '\\n' + 'Categories: ' + articles_df['Category'] + ' ' + 'Keywords: ' + articles_df['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['reliability_article'] == 0]\n",
    "test_df = df[df['reliability_article'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 1) (82, 1)\n",
      "(328, 1) (82, 1)\n"
     ]
    }
   ],
   "source": [
    "# Select input text\n",
    "X = train_df[['input_text']]\n",
    "\n",
    "# Select target labels directly from the binary topic columns\n",
    "y = train_df[['topic_ij']]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_val.shape)  # Shapes of the input texts\n",
    "print(y_train.shape, y_val.shape)    # Shapes of the binary labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned RobBERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eklk/.conda/envs/mistralenv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for 5 epochs...\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:08<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3094\n",
      "New best model found! Saving...\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2274\n",
      "New best model found! Saving...\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2431\n",
      "Epoch: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2999\n",
      "Epoch: 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3525\n",
      "\n",
      "Training for 10 epochs...\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4164\n",
      "Epoch: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4825\n",
      "Epoch: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4653\n",
      "Epoch: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:07<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:00<00:00, 29.36it/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4643\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with learning rate: 2e-06\n",
      "\n",
      "Training for 5 epochs...\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:12<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:01<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5804\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:25<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:02<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3520\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:25<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:02<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3096\n",
      "Epoch: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:26<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:03<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2984\n",
      "Early stopping triggered.\n",
      "\n",
      "Training for 10 epochs...\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:34<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:03<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2823\n",
      "Epoch: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:34<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:03<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2715\n",
      "Epoch: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:34<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:03<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2471\n",
      "Epoch: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:34<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:03<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2314\n",
      "Early stopping triggered.\n",
      "Best model saved with Val loss: 0.2274\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [2e-5, 2e-6]\n",
    "accumulation_steps = 4  # This simulates a batch size of 16 by accumulating over 2 steps with batch size 8\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 4  # Number of epochs to wait before stopping if no improvement\n",
    "epochs_to_test = [5, 10]  # Epochs to test\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", cache_dir=huggingface_cache_dir)\n",
    "\n",
    "# Encode data\n",
    "def encode(docs):\n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, padding='max_length',\n",
    "                                                return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "train_input_ids, train_att_masks = encode(X_train['input_text'].tolist())\n",
    "valid_input_ids, valid_att_masks = encode(X_val['input_text'].tolist())\n",
    "train_y = torch.LongTensor(y_train.values.squeeze())\n",
    "valid_y = torch.LongTensor(y_val.values.squeeze())\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=4)  # Small batch size\n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=4)\n",
    "\n",
    "# Loop through specified learning rates\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Initialize the model for each learning rate\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", \n",
    "                                                                num_labels=2, \n",
    "                                                                cache_dir=huggingface_cache_dir)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Loop through specified epochs\n",
    "    for epochs in epochs_to_test:\n",
    "        print(f\"\\nTraining for {epochs} epochs...\")\n",
    "        patience_counter = 0  # To track epochs without improvement\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            print(f\"Epoch: {epoch_num + 1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "                input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                \n",
    "                # CrossEntropyLoss automatically applies softmax, so logits are passed directly\n",
    "                output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "                logits = output.logits  # Get logits directly\n",
    "                \n",
    "                loss = criterion(logits, labels)  # Compute loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss = loss / accumulation_steps  # Scale the loss\n",
    "                loss.backward()  # Backpropagate the loss\n",
    "\n",
    "                # Gradient accumulation\n",
    "                if (step_num + 1) % accumulation_steps == 0:\n",
    "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            # Average training loss for this epoch\n",
    "            train_loss /= len(train_dataloader)\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            valid_pred = []\n",
    "            with torch.no_grad():\n",
    "                for step_num_e, batch_data in enumerate(tqdm(valid_dataloader, desc='Validation')):\n",
    "                    input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                    output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "                    logits = output.logits\n",
    "                    \n",
    "                    loss = criterion(logits, labels)  # Compute loss\n",
    "                    valid_loss += loss.item()\n",
    "                    valid_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "            # Average validation loss for this epoch\n",
    "            valid_loss /= len(valid_dataloader)\n",
    "            print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                patience_counter = 0  # Reset patience counter if we have improvement\n",
    "                print(\"New best model found! Saving...\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break  # Stop training if no improvement\n",
    "\n",
    "# After training, save the best model\n",
    "torch.save(best_model_state, 'results/topic_ij_robbert_base_best_model.pt')\n",
    "print(f\"Best model saved with Val loss: {best_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\",\n",
    "                                                              num_labels=2, cache_dir=huggingface_cache_dir)\n",
    "\n",
    "model.load_state_dict(torch.load('results/topic_ij_robbert_base_best_model.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:03<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_topic_ij       0.95      0.99      0.97        74\n",
      "    topic_ij       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.94        82\n",
      "   macro avg       0.87      0.74      0.79        82\n",
      "weighted avg       0.93      0.94      0.93        82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "valid_loss = 0\n",
    "valid_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num_e, batch_data in enumerate(tqdm(valid_dataloader, desc='Validation')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        # During validation, no need to pass labels to the model\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        logits = output.logits\n",
    "        \n",
    "        # Compute validation loss manually (if needed)\n",
    "        loss = criterion(logits, labels)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # Store the logits for all validation examples\n",
    "        valid_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "# Average validation loss\n",
    "valid_loss /= len(valid_dataloader)\n",
    "print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "# Check if valid_pred has any entries before concatenation\n",
    "if valid_pred:\n",
    "    valid_pred = np.concatenate(valid_pred)  # Concatenate predictions from batches\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    valid_pred_softmax = torch.softmax(torch.tensor(valid_pred), dim=1).numpy()\n",
    "\n",
    "    # Get the class predictions (0 or 1) based on the higher probability\n",
    "    valid_pred_labels = np.argmax(valid_pred_softmax, axis=1)\n",
    "\n",
    "    # Flatten ground truth for classification report\n",
    "    y_true = valid_y.numpy()\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_true, valid_pred_labels, \n",
    "                                target_names=['not_topic_ij', 'topic_ij'],\n",
    "                                zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the reliability df\n",
    "reliability_df = pd.read_csv('data/reliability_topics_elif.csv',\n",
    "                             sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "reliability_df['article_id'] = reliability_df['article_id'].astype(int)\n",
    "\n",
    "reliability_df.head()\n",
    "\n",
    "# merge reliability_df with articles_df\n",
    "reliability_df = pd.merge(reliability_df, articles_df, on='article_id', how = 'left')\n",
    "\n",
    "# make an input text, combining Title, Text, Category and Keywords and before Category add string 'Categories: ' and before Keywords add string 'Keywords: ' if Category and Keywords are empty skip them\n",
    "reliability_df['Category'] = reliability_df['Category'].fillna('')\n",
    "reliability_df['Keywords'] = reliability_df['Keywords'].fillna('')\n",
    "reliability_df['input_text'] = reliability_df['Text'] + '\\n' + 'Categories: ' + reliability_df['Category'] + ' ' + 'Keywords: ' + reliability_df['Keywords']\n",
    "\n",
    "# make a test_df for all coders separately\n",
    "test_df_elif = reliability_df[reliability_df['coder'] == 'Elif Kilik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 30/30 [00:04<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_topic_ij       0.89      0.84      0.87       107\n",
      "    topic_ij       0.11      0.15      0.12        13\n",
      "\n",
      "    accuracy                           0.77       120\n",
      "   macro avg       0.50      0.50      0.50       120\n",
      "weighted avg       0.81      0.77      0.79       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run it on test data\n",
    "# Select input text\n",
    "\n",
    "X_test = test_df_elif[['input_text']]\n",
    "# Select target labels directly from the binary topic columns\n",
    "y_test = test_df_elif[['topic_ij']]\n",
    "# Encode data\n",
    "test_input_ids, test_att_masks = encode(X_test['input_text'].tolist())\n",
    "test_y = torch.LongTensor(y_test.values.squeeze())\n",
    "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=4)\n",
    "\n",
    "# predict on test data\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num_e, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        # During validation, no need to pass labels to the model\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        logits = output.logits\n",
    "        \n",
    "        # Compute validation loss manually (if needed)\n",
    "        loss = criterion(logits, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store the logits for all validation examples\n",
    "        test_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "# get predictions to compute classification report\n",
    "\n",
    "# Average validation loss\n",
    "test_loss /= len(test_dataloader)\n",
    "\n",
    "# Check if valid_pred has any entries before concatenation\n",
    "\n",
    "if test_pred:\n",
    "    test_pred = np.concatenate(test_pred)  # Concatenate predictions from batches\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    test_pred_softmax = torch.softmax(torch.tensor(test_pred), dim=1).numpy()\n",
    "\n",
    "    # Get the class predictions\n",
    "    test_pred_labels = np.argmax(test_pred_softmax, axis=1)\n",
    "\n",
    "    # Flatten ground truth for classification report\n",
    "    y_true = test_y.numpy()\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    print(classification_report(y_true, test_pred_labels,\n",
    "                                target_names=['not_topic_ij', 'topic_ij'],\n",
    "                                zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistralenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
