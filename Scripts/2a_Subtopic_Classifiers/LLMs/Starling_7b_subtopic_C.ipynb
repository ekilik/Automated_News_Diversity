{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/\")\n",
    "\n",
    "huggingface_cache_dir = 'model_starling'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = 'berkeley-nest/Starling-LM-7B-alpha'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=huggingface_cache_dir\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    cache_dir=huggingface_cache_dir\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=huggingface_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "Memory Usage: 3.96816349029541 GB\n",
      "Max Memory Usage: 3.968377113342285 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0,  # 'randomness' of outputs, 0.0 is not possible, so use a very small number\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    article_ids = []\n",
    "    subtopic_discussed_values = []\n",
    "\n",
    "    # Define regex pattern for article_id and about_covid with optional double quotes\n",
    "    article_id_pattern = r'\"article_id\"\\s*:\\s*\"?(\\d+)\"?'\n",
    "    subtopic_discussed_pattern = r'\"subtopic_discussed\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Use regex to find matches for article_id\n",
    "        article_id_match = re.search(article_id_pattern, string_data)\n",
    "\n",
    "        # Use regex to find matches for about_covid\n",
    "        subtopic_discussed_match = re.search(subtopic_discussed_pattern, string_data)\n",
    "\n",
    "        # Extract values from the regex matches\n",
    "        article_id = int(article_id_match.group(1)) if article_id_match else None\n",
    "        subtopic_discussed = int(subtopic_discussed_match.group(1)) if subtopic_discussed_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        article_ids.append(article_id)\n",
    "        subtopic_discussed_values.append(subtopic_discussed)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        \"subtopic_discussed\": subtopic_discussed_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(system_prompt, input_prompt, main_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": input_prompt + main_prompt},\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NOS articles annotated by the researcher\n",
    "df = pd.read_csv('NOS/nos_analysis/topic_tests_reliability_data/reliability_topics_elif.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)\n",
    "\n",
    "df = df[df['about_covid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df including text, category, keywords \n",
    "articles_df = pd.read_csv('NOS/nos_analysis/data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique articles from researcher annotations\n",
    "unique_articles = df[['article_id', 'Keywords', 'Category', 'Text']].drop_duplicates()\n",
    "print(unique_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Topic C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article substantially discusses the subtopic \"Coronavirus (COVID-19) tests and testing procedures\".  \n",
    "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = \"\"\"\n",
    "Read the following article with the ID {article_id}: {text}. \\n\n",
    "This article falls under the categories: {category} and contains the keywords: {keywords}.\n",
    "\"\"\"\n",
    "\n",
    "main_prompt = \"\"\"\n",
    "Take a moment to understand the article. \n",
    "\n",
    "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
    "\n",
    "Carefully analyze if the article contains substantial discussion of \"Coronavirus (COVID-19) tests and testing procedures\" based on the definition above.\n",
    " \n",
    "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
    "\n",
    "Output your results in a JSON format with keys 'article_id' and 'subtopic_discussed', where the id of the article and and your answer are the values. \n",
    "Follow the example output format provided. Do not include any additional information or explanation.\n",
    "\n",
    "Example Output (JSON format):\n",
    "{{\n",
    "    \"article_id\": \"2000000\",\n",
    "    \"subtopic_discussed\": \"1\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot - Topic C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>GPT4 Correct System: \n",
      "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article substantially discusses the subtopic \"Coronavirus (COVID-19) tests and testing procedures\".  \n",
      "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
      "\n",
      "<|end_of_turn|>GPT4 Correct User: \n",
      "Read the following article with the ID {article_id}: {text}. \n",
      "\n",
      "This article falls under the categories: {category} and contains the keywords: {keywords}.\n",
      "\n",
      "Take a moment to understand the article. \n",
      "\n",
      "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
      "\n",
      "Carefully analyze if the article contains substantial discussion of \"Coronavirus (COVID-19) tests and testing procedures\" based on the definition above.\n",
      " \n",
      "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
      "\n",
      "Output your results in a JSON format with keys 'article_id' and 'subtopic_discussed', where the id of the article and and your answer are the values. \n",
      "Follow the example output format provided. Do not include any additional information or explanation.\n",
      "\n",
      "Example Output (JSON format):\n",
      "{{\n",
      "    \"article_id\": \"2000000\",\n",
      "    \"subtopic_discussed\": \"1\"\n",
      "}}\n",
      "<|end_of_turn|>GPT4 Correct Assistant:\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt_c = zero_shot_prompt_messages(system_prompt, input_prompt, main_prompt)\n",
    "print(zero_shot_prompt_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"text\", \"category\", \"keywords\"],\n",
    "    template=zero_shot_prompt_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'category', 'keywords', 'text'], template='<s>GPT4 Correct System: \\nYou are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article substantially discusses the subtopic \"Coronavirus (COVID-19) tests and testing procedures\".  \\nSubstantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \\n\\n<|end_of_turn|>GPT4 Correct User: \\nRead the following article with the ID {article_id}: {text}. \\n\\nThis article falls under the categories: {category} and contains the keywords: {keywords}.\\n\\nTake a moment to understand the article. \\n\\nRemember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\\n\\nCarefully analyze if the article contains substantial discussion of \"Coronavirus (COVID-19) tests and testing procedures\" based on the definition above.\\n \\nAssign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\\n\\nOutput your results in a JSON format with keys \\'article_id\\' and \\'subtopic_discussed\\', where the id of the article and and your answer are the values. \\nFollow the example output format provided. Do not include any additional information or explanation.\\n\\nExample Output (JSON format):\\n{{\\n    \"article_id\": \"2000000\",\\n    \"subtopic_discussed\": \"1\"\\n}}\\n<|end_of_turn|>GPT4 Correct Assistant:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fbca2afc7f0>), output_key='article_id, subtopic_discussed')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_c = LLMChain(llm = llm, prompt = prompt_template, output_key=\"article_id, subtopic_discussed\")\n",
    "chain_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot_c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "for index, row in unique_articles.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    text = row['Text']\n",
    "    category = row['Category']\n",
    "    keywords = row['Keywords']\n",
    "\n",
    "\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"text\": text,\n",
    "            \"category\": category,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "        \n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_one.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot_c.append(generated_text)  \n",
    "\n",
    "# CPU times: 3min 24s, sys: 19.2 s, total: 3min 44s\n",
    "# Wall time: 3min 44s \n",
    "# output cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = regex_extract_to_dataframe(generated_text_zeroshot_c)\n",
    "df_topic_c = pd.DataFrame(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_c = df_topic_c.rename(columns={'topic_discussed': 'topic_c_pred'})\n",
    "df_topic_c['article_id'] = df_topic_c['article_id'].astype(int)\n",
    "df_topic_c['topic_c_pred'] = df_topic_c['topic_c_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elif = df[df['coder'] == 'Elif Kilik']\n",
    "df_coded_elif = df_elif[['article_id', 'topic_c']]\n",
    "df_zeroshot_merged_elif = pd.merge(df_coded_elif, df_topic_c, how='left', on=\"article_id\")\n",
    "df_zeroshot_merged_elif = df_zeroshot_merged_elif.dropna()\n",
    "df_zeroshot_merged_elif['topic_c_pred'] = df_zeroshot_merged_elif['topic_c_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        66\n",
      "           1       1.00      0.42      0.59        12\n",
      "\n",
      "    accuracy                           0.91        78\n",
      "   macro avg       0.95      0.71      0.77        78\n",
      "weighted avg       0.92      0.91      0.89        78\n",
      "\n",
      "Cohen's Kappa: 0.5472636815920398\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_zeroshot_merged_elif['topic_c'], df_zeroshot_merged_elif['topic_c_pred']))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(df_zeroshot_merged_elif['topic_c'], df_zeroshot_merged_elif['topic_c_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
