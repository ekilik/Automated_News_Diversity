{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/\")\n",
    "\n",
    "huggingface_cache_dir = 'model_mistral'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "# set model access token for huggingface\n",
    "hf_token = 'hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map='auto',\n",
    "    token=hf_token,\n",
    "    cache_dir=huggingface_cache_dir)\n",
    "model.eval()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_token, cache_dir=huggingface_cache_dir)\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "Memory Usage: 13.488792419433594 GB\n",
      "Max Memory Usage: 13.488792419433594 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0,  # 'randomness' of outputs, 0.0 is not possible, so use a very small number\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    article_ids = []\n",
    "    topic_discussed_values = []\n",
    "\n",
    "    # Define regex pattern for article_id and about_covid with optional double quotes\n",
    "    article_id_pattern = r'\"article_id\"\\s*:\\s*\"?(\\d+)\"?'\n",
    "    topic_discussed_pattern = r'\"subtopic_discussed\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Clean the string_data: change \"\\\\_\" pattern to \"_\"\n",
    "        string_data = string_data.replace('\\\\', '',)\n",
    "\n",
    "        # Use regex to find matches for article_id\n",
    "        article_id_match = re.search(article_id_pattern, string_data)\n",
    "\n",
    "        # Use regex to find matches for about_covid\n",
    "        topic_discussed_match = re.search(topic_discussed_pattern, string_data)\n",
    "\n",
    "        # Extract values from the regex matches\n",
    "        article_id = int(article_id_match.group(1)) if article_id_match else None\n",
    "        topic_discussed = int(topic_discussed_match.group(1)) if topic_discussed_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        article_ids.append(article_id)\n",
    "        topic_discussed_values.append(topic_discussed)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        \"topic_discussed\": topic_discussed_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(prompt_text):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NOS articles annotated by the researcher\n",
    "df = pd.read_csv('NOS/nos_analysis/topic_tests_reliability_data/reliability_topics_elif.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)\n",
    "\n",
    "df = df[df['about_covid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df including text, category, keywords \n",
    "articles_df = pd.read_csv('NOS/nos_analysis/data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 4)\n"
     ]
    }
   ],
   "source": [
    "# unique articles from researcher annotations\n",
    "unique_articles = df[['article_id', 'Keywords', 'Category', 'Text']].drop_duplicates()\n",
    "print(unique_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Topic A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article after <<<>>> substantially discusses the subtopic \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\". \n",
    "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
    "\n",
    "The discussion on the subtopic \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\" may focus on one or more of the following aspects:\n",
    "\n",
    "- The current status and spread of the Coronavirus/COVID-19 pandemic.\n",
    "- Any discussion on the number/s or percentage/s of:\n",
    "    - confirmed coronavirus/COVID-19 cases, people who got infected with coronavirus, \n",
    "    - people who got tested for coronavirus, positive/negative coronavirus test results, \n",
    "    - COVID-19/corona patients, hospitalized patients, recovered patients, percentage of intensive care beds used, IC capacity, \n",
    "    - people who died because of coronavirus\\COVID-19, death rates, etc.\n",
    "    - Regional, national, or international statistical data regarding coronavirus or COVID-19 cases. \n",
    "- Discussions around the decreasing/increasing corona numbers or percentages.\n",
    "- Discussions about the reproduction number (Reproductie/R getal).\n",
    "- Any other relevant national or international statistics/data about the Coronavirus/COVID-19 pandemic.\n",
    "\n",
    "Read the following news article with the ID {article_id}: <<< {text} >>> \\n\n",
    "This article falls under the categories: {category} and contains the keywords: {keywords}. \n",
    "\n",
    "Take a moment to understand the article. \n",
    "\n",
    "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
    "\n",
    "Carefully analyze if the article contains substantial discussion of \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\" based on the definition above. \n",
    "\n",
    "If yes, respond with \"1\". If no, respond with \"0\". \n",
    "\n",
    "Your response should be in the following JSON format, without any additional explanations or notes:\n",
    "\n",
    "{{\n",
    "  \"article_id\": \"2000000\",\n",
    "  \"subtopic_discussed\": 0/1\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot - Topic A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] \n",
      "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article after <<<>>> substantially discusses the subtopic \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\". \n",
      "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
      "\n",
      "The discussion on the subtopic \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\" may focus on one or more of the following aspects:\n",
      "\n",
      "- The current status and spread of the Coronavirus/COVID-19 pandemic.\n",
      "- Any discussion on the number/s or percentage/s of:\n",
      "    - confirmed coronavirus/COVID-19 cases, people who got infected with coronavirus, \n",
      "    - people who got tested for coronavirus, positive/negative coronavirus test results, \n",
      "    - COVID-19/corona patients, hospitalized patients, recovered patients, percentage of intensive care beds used, IC capacity, \n",
      "    - people who died because of coronavirus\\COVID-19, death rates, etc.\n",
      "    - Regional, national, or international statistical data regarding coronavirus or COVID-19 cases. \n",
      "- Discussions around the decreasing/increasing corona numbers or percentages.\n",
      "- Discussions about the reproduction number (Reproductie/R getal).\n",
      "- Any other relevant national or international statistics/data about the Coronavirus/COVID-19 pandemic.\n",
      "\n",
      "Read the following news article with the ID {article_id}: <<< {text} >>> \n",
      "\n",
      "This article falls under the categories: {category} and contains the keywords: {keywords}. \n",
      "\n",
      "Take a moment to understand the article. \n",
      "\n",
      "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
      "\n",
      "Carefully analyze if the article contains substantial discussion of \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\" based on the definition above. \n",
      "\n",
      "If yes, respond with \"1\". If no, respond with \"0\". \n",
      "\n",
      "Your response should be in the following JSON format, without any additional explanations or notes:\n",
      "\n",
      "{{\n",
      "  \"article_id\": \"2000000\",\n",
      "  \"subtopic_discussed\": 0/1\n",
      "}}\n",
      " [/INST]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt_a = zero_shot_prompt_messages(prompt_text)\n",
    "print(zero_shot_prompt_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"text\", \"category\", \"keywords\"],\n",
    "    template=zero_shot_prompt_a\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'category', 'keywords', 'text'], template='<s> [INST] \\nYou are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article after <<<>>> substantially discusses the subtopic \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\". \\nSubstantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \\n\\nThe discussion on the subtopic \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\" may focus on one or more of the following aspects:\\n\\n- The current status and spread of the Coronavirus/COVID-19 pandemic.\\n- Any discussion on the number/s or percentage/s of:\\n    - confirmed coronavirus/COVID-19 cases, people who got infected with coronavirus, \\n    - people who got tested for coronavirus, positive/negative coronavirus test results, \\n    - COVID-19/corona patients, hospitalized patients, recovered patients, percentage of intensive care beds used, IC capacity, \\n    - people who died because of coronavirus\\\\COVID-19, death rates, etc.\\n    - Regional, national, or international statistical data regarding coronavirus or COVID-19 cases. \\n- Discussions around the decreasing/increasing corona numbers or percentages.\\n- Discussions about the reproduction number (Reproductie/R getal).\\n- Any other relevant national or international statistics/data about the Coronavirus/COVID-19 pandemic.\\n\\nRead the following news article with the ID {article_id}: <<< {text} >>> \\n\\nThis article falls under the categories: {category} and contains the keywords: {keywords}. \\n\\nTake a moment to understand the article. \\n\\nRemember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\\n\\nCarefully analyze if the article contains substantial discussion of \"Current status on the spread of the COVID-19 pandemic and pandemic statistics\" based on the definition above. \\n\\nIf yes, respond with \"1\". If no, respond with \"0\". \\n\\nYour response should be in the following JSON format, without any additional explanations or notes:\\n\\n{{\\n  \"article_id\": \"2000000\",\\n  \"subtopic_discussed\": 0/1\\n}}\\n [/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x715b105282e0>), output_key='article_id, subtopic_discussed')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_a = LLMChain(llm = llm, prompt = prompt_template, output_key=\"article_id, subtopic_discussed\")\n",
    "chain_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot_a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for index, row in unique_articles.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    text = row['Text']\n",
    "    category = row['Category']\n",
    "    keywords = row['Keywords']\n",
    "\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"text\": text,\n",
    "            \"category\": category,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_a.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot_a.append(generated_text)    \n",
    "\n",
    "# CPU times: user 2min 2s, sys: 2.35 s, total: 2min 4s\n",
    "# Wall time: 2min 4s\n",
    "# output cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = regex_extract_to_dataframe(generated_text_zeroshot_a)\n",
    "df_topic_a = pd.DataFrame(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_a = df_topic_a.rename(columns={'topic_discussed': 'topic_a_pred'})\n",
    "df_topic_a['article_id'] = df_topic_a['article_id'].astype(int)\n",
    "df_topic_a['about_covid_pred'] = df_topic_a['topic_a_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elif = df[df['coder'] == 'Elif Kilik']\n",
    "df_coded_elif = df_elif[['article_id', 'topic_a']]\n",
    "df_zeroshot_merged_elif = pd.merge(df_coded_elif, df_topic_a, how='left', on=\"article_id\")\n",
    "df_zeroshot_merged_elif = df_zeroshot_merged_elif.dropna()\n",
    "df_zeroshot_merged_elif['topic_a_pred'] = df_zeroshot_merged_elif['topic_a_pred'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91        57\n",
      "           1       0.72      0.86      0.78        21\n",
      "\n",
      "    accuracy                           0.87        78\n",
      "   macro avg       0.83      0.87      0.85        78\n",
      "weighted avg       0.88      0.87      0.88        78\n",
      "\n",
      "Cohen's Kappa: 0.6926713947990544\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_zeroshot_merged_elif['topic_a'], df_zeroshot_merged_elif['topic_a_pred']))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(df_zeroshot_merged_elif['topic_a'], df_zeroshot_merged_elif['topic_a_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
