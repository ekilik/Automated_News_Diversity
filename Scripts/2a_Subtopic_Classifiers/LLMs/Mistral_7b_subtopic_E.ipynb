{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/\")\n",
    "\n",
    "huggingface_cache_dir = 'model_mistral'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "# set model access token for huggingface\n",
    "hf_token = 'hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map='auto',\n",
    "    token=hf_token,\n",
    "    cache_dir=huggingface_cache_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_token, cache_dir=huggingface_cache_dir)\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "Memory Usage: 13.488792419433594 GB\n",
      "Max Memory Usage: 13.488792419433594 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.0,  \n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # how much to avoid repeating the same word\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    article_ids = []\n",
    "    topic_discussed_values = []\n",
    "\n",
    "    # Define regex pattern for article_id and about_covid with optional double quotes\n",
    "    article_id_pattern = r'\"article_id\"\\s*:\\s*\"?(\\d+)\"?'\n",
    "    topic_discussed_pattern = r'\"subtopic_discussed\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Clean the string_data: change \"\\\\_\" pattern to \"_\"\n",
    "        string_data = string_data.replace('\\\\_', '_',)\n",
    "\n",
    "        # Use regex to find matches for article_id\n",
    "        article_id_match = re.search(article_id_pattern, string_data)\n",
    "\n",
    "        # Use regex to find matches for about_covid\n",
    "        topic_discussed_match = re.search(topic_discussed_pattern, string_data)\n",
    "\n",
    "        # Extract values from the regex matches\n",
    "        article_id = int(article_id_match.group(1)) if article_id_match else None\n",
    "        topic_discussed = int(topic_discussed_match.group(1)) if topic_discussed_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        article_ids.append(article_id)\n",
    "        topic_discussed_values.append(topic_discussed)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        \"subtopic_discussed\": topic_discussed_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(prompt_text):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NOS articles annotated by the researcher\n",
    "df = pd.read_csv('NOS/nos_analysis/topic_tests_reliability_data/reliability_topics_elif.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)\n",
    "\n",
    "df = df[df['about_covid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df including text, category, keywords \n",
    "articles_df = pd.read_csv('NOS/nos_analysis/data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique articles from researcher annotations\n",
    "unique_articles = df[['article_id', 'Keywords', 'Category', 'Text']].drop_duplicates()\n",
    "print(unique_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Topic E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article after <<<>>> substantially discusses the subtopic \"Long COVID and the long-term impact of coronavirus on physical health\".\n",
    "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
    "\n",
    "Discussion of \"Long COVID and the long-term impact of coronavirus on physical health\" may focus on one or more of the following aspects:\n",
    "\n",
    "- Long COVID and the long-term impact of coronavirus on health.\n",
    "- The persistent symptoms and complications of long-covid that occur after recovery from the acute stage of COVID-19, including fatigue, shortness of breath, and cognitive impairment.\n",
    "- Long-term impact of the COVID-19 disease or long-covid on the daily lives of affected persons.\n",
    "- Prevalence among different demographics and risk factors for having long covid.\n",
    "- Research and discussions on long-covid, diagnostic challenges due to symptom overlap.\n",
    "- Treatment options for long covid, including pharmacological and holistic approaches.\n",
    "- Public health implications of long term effects of the coronavirus and long covid, affecting healthcare systems and society.\n",
    "- Groups who are more likely to experience long covid and the long-term impact of coronavirus on physical health.\n",
    "- The healthcare workers who are at risk of long covid, and the initiatives to help those who already have long covid.\n",
    "\n",
    "Read the following news article with the ID {article_id}: <<< {text} >>>\n",
    "\n",
    "This article falls under the categories: {category} and contains the keywords: {keywords}. \n",
    "\n",
    "Take a moment to understand the article. \n",
    "\n",
    "Carefully analyze if the article contains substantial discussion of \"Long COVID and the long-term impact of coronavirus on physical health\" based on the definition above.\n",
    " \n",
    "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
    "\n",
    "Your response should be in the following JSON format, without any additional explanations or notes:\n",
    "\n",
    "{{\n",
    "    \"article_id\": \"2000000\",\n",
    "  \"subtopic_discussed\": 0/1\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot - Topic E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] \n",
      "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article after <<<>>> substantially discusses the subtopic \"Long COVID and the long-term impact of coronavirus on physical health\".\n",
      "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
      "\n",
      "Discussion of \"Long COVID and the long-term impact of coronavirus on physical health\" may focus on one or more of the following aspects:\n",
      "\n",
      "- Long COVID and the long-term impact of coronavirus on health.\n",
      "- The persistent symptoms and complications of long-covid that occur after recovery from the acute stage of COVID-19, including fatigue, shortness of breath, and cognitive impairment.\n",
      "- Long-term impact of the COVID-19 disease or long-covid on the daily lives of affected persons.\n",
      "- Prevalence among different demographics and risk factors for having long covid.\n",
      "- Research and discussions on long-covid, diagnostic challenges due to symptom overlap.\n",
      "- Treatment options for long covid, including pharmacological and holistic approaches.\n",
      "- Public health implications of long term effects of the coronavirus and long covid, affecting healthcare systems and society.\n",
      "- Groups who are more likely to experience long covid and the long-term impact of coronavirus on physical health.\n",
      "- The healthcare workers who are at risk of long covid, and the initiatives to help those who already have long covid.\n",
      "\n",
      "Read the following news article with the ID {article_id}: <<< {text} >>>\n",
      "\n",
      "This article falls under the categories: {category} and contains the keywords: {keywords}. \n",
      "\n",
      "Take a moment to understand the article. \n",
      "\n",
      "Carefully analyze if the article contains substantial discussion of \"Long COVID and the long-term impact of coronavirus on physical health\" based on the definition above.\n",
      " \n",
      "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
      "\n",
      "Your response should be in the following JSON format, without any additional explanations or notes:\n",
      "\n",
      "{{\n",
      "    \"article_id\": \"2000000\",\n",
      "  \"subtopic_discussed\": 0/1\n",
      "}}\n",
      " [/INST]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = zero_shot_prompt_messages(prompt_text)\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"text\", \"category\", \"keywords\"],\n",
    "    template=zero_shot_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'category', 'keywords', 'text'], template='<s> [INST] \\nYou are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article after <<<>>> substantially discusses the subtopic \"Long COVID and the long-term impact of coronavirus on physical health\".\\nSubstantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \\n\\nDiscussion of \"Long COVID and the long-term impact of coronavirus on physical health\" may focus on one or more of the following aspects:\\n\\n- Long COVID and the long-term impact of coronavirus on health.\\n- The persistent symptoms and complications of long-covid that occur after recovery from the acute stage of COVID-19, including fatigue, shortness of breath, and cognitive impairment.\\n- Long-term impact of the COVID-19 disease or long-covid on the daily lives of affected persons.\\n- Prevalence among different demographics and risk factors for having long covid.\\n- Research and discussions on long-covid, diagnostic challenges due to symptom overlap.\\n- Treatment options for long covid, including pharmacological and holistic approaches.\\n- Public health implications of long term effects of the coronavirus and long covid, affecting healthcare systems and society.\\n- Groups who are more likely to experience long covid and the long-term impact of coronavirus on physical health.\\n- The healthcare workers who are at risk of long covid, and the initiatives to help those who already have long covid.\\n\\nRead the following news article with the ID {article_id}: <<< {text} >>>\\n\\nThis article falls under the categories: {category} and contains the keywords: {keywords}. \\n\\nTake a moment to understand the article. \\n\\nCarefully analyze if the article contains substantial discussion of \"Long COVID and the long-term impact of coronavirus on physical health\" based on the definition above.\\n \\nAssign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\\n\\nYour response should be in the following JSON format, without any additional explanations or notes:\\n\\n{{\\n    \"article_id\": \"2000000\",\\n  \"subtopic_discussed\": 0/1\\n}}\\n [/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7a60b7a58be0>), output_key='article_id, subtopic_discussed')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_one = LLMChain(llm = llm, prompt = prompt_template, output_key=\"article_id, subtopic_discussed\")\n",
    "chain_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot_e = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "for index, row in unique_articles.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    text = row['Text']\n",
    "    category = row['Category']\n",
    "    keywords = row['Keywords']\n",
    "\n",
    "\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"text\": text,\n",
    "            \"category\": category,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_one.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot_e.append(generated_text)     \n",
    "\n",
    "# CPU times: user 4min 18s, sys: 20.7 s, total: 4min 38s\n",
    "# Wall time: 4min 38s\n",
    "# output cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = regex_extract_to_dataframe(generated_text_zeroshot_e)\n",
    "df_topic_e = pd.DataFrame(json_list)\n",
    "# drop nan\n",
    "df_topic_e = df_topic_e.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_e = df_topic_e.rename(columns={'topic_discussed': 'topic_e_pred'})\n",
    "df_topic_e['article_id'] = df_topic_e['article_id'].astype(int)\n",
    "df_topic_e['topic_e_pred'] = df_topic_e['topic_e_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elif = df[df['coder'] == 'Elif Kilik']\n",
    "df_coded_elif = df_elif[['article_id', 'topic_e']]\n",
    "df_zeroshot_merged_elif = pd.merge(df_coded_elif, df_topic_e, how='left', on=\"article_id\")\n",
    "df_zeroshot_merged_elif = df_zeroshot_merged_elif.dropna()\n",
    "df_zeroshot_merged_elif['topic_e_pred'] = df_zeroshot_merged_elif['topic_e_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        70\n",
      "           1       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.99        78\n",
      "   macro avg       0.99      0.94      0.96        78\n",
      "weighted avg       0.99      0.99      0.99        78\n",
      "\n",
      "Cohen's Kappa: 0.9262759924385633\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_zeroshot_merged_elif['topic_e'], df_zeroshot_merged_elif['topic_e_pred']))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(df_zeroshot_merged_elif['topic_e'], df_zeroshot_merged_elif['topic_e_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistralenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
