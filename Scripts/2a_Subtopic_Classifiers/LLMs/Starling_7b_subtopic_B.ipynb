{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages & Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clear_autocast_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/\")\n",
    "\n",
    "huggingface_cache_dir = 'model'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [03:56<00:00, 78.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = 'berkeley-nest/Starling-LM-7B-alpha'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=huggingface_cache_dir\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    cache_dir=huggingface_cache_dir\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=huggingface_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "Memory Usage: 3.96816349029541 GB\n",
      "Max Memory Usage: 3.968377113342285 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is not possible, so use a very small number\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    article_ids = []\n",
    "    subtopic_discussed_values = []\n",
    "\n",
    "    # Define regex pattern for article_id and about_covid with optional double quotes\n",
    "    article_id_pattern = r'\"article_id\"\\s*:\\s*\"?(\\d+)\"?'\n",
    "    subtopic_discussed_pattern = r'\"subtopic_discussed\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Use regex to find matches for article_id\n",
    "        article_id_match = re.search(article_id_pattern, string_data)\n",
    "\n",
    "        # Use regex to find matches for about_covid\n",
    "        subtopic_discussed_match = re.search(subtopic_discussed_pattern, string_data)\n",
    "\n",
    "        # Extract values from the regex matches\n",
    "        article_id = int(article_id_match.group(1)) if article_id_match else None\n",
    "        subtopic_discussed = int(subtopic_discussed_match.group(1)) if subtopic_discussed_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        article_ids.append(article_id)\n",
    "        subtopic_discussed_values.append(subtopic_discussed)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        \"subtopic_discussed\": subtopic_discussed_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(system_prompt, input_prompt, main_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": input_prompt + main_prompt},\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NOS articles annotated by the researcher\n",
    "df = pd.read_csv('NOS/nos_analysis/topic_tests_reliability_data/reliability_topics_elif.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)\n",
    "\n",
    "df = df[df['about_covid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df including text, category, keywords \n",
    "articles_df = pd.read_csv('NOS/nos_analysis/data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 4)\n"
     ]
    }
   ],
   "source": [
    "# unique articles from researcher annotations\n",
    "unique_articles = df[['article_id', 'Keywords', 'Category', 'Text']].drop_duplicates()\n",
    "print(unique_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Topic B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article substantially discusses the subtopic \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\". \n",
    "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
    "\n",
    "The discussion on the subtopic \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\" may focus on one or more of the following aspects:\n",
    "\n",
    "- The governmental measures taken against the spread of the Coronavirus/COVID-19 pandemic, for example: social distancing, one and a half meter distance rule, lockdown, curfews, staying at home, quarantine, closing of restaurants, closing of shops, closing of schools & universities, travel restrictions, travel bans, border controls, guidelines imposed by different countries regarding domestic and international travel, measures effecting entertainment industry (closing of clubs, bars, rules about events and festivals), and other measures taken during the coronavirus pandemic.\n",
    "- Extensions or tightening of the measures taken during the Coronavirus/COVID-19 pandemic. \n",
    "- Precautionary measures and advice to combat the Coronavirus/COVID-19 pandemic, for example washing hands, wearing face masks, etc.\n",
    "- Relaxations (versoepelingen) and/or abolishing of the corona measures, for example: the end of the lockdown, the end of the curfew, the end of the one and a half meter distance rule, the end of the obligation to wear face masks, the end of the obligation to work from home, opening the restaurants, opening the shops, opening the schools & universities, lifting of travel restrictions, lifting of travel bans, lifting of border controls, guidelines imposed by different countries regarding domestic and international travel, measures effecting entertainment industry (opening of clubs, bars, rules about events and festivals).\n",
    "- Other discussions regarding the measures taken during the Coronavirus/COVID-19 pandemic, rules and regulations during the pandemic. \\n\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = \"\"\"\n",
    "Read the following article with the ID {article_id}: {text} \\n\n",
    "This article falls under the categories: {category} and contains the keywords: {keywords}.\n",
    "\"\"\"\n",
    "\n",
    "main_prompt = \"\"\"\n",
    "Take a moment to understand the article. \n",
    "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
    "\n",
    "Carefully analyze if the article contains substantial discussion of \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\" based on the definition above.\n",
    "\n",
    "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
    "\n",
    "Output your results in a JSON format with keys 'article_id' and 'subtopic_discussed', where the id of the article and and your answer are the values. \n",
    "Follow the example output format provided. Do not include any additional information or explanation.\n",
    "\n",
    "Example Output (JSON format):\n",
    "{{\n",
    "    \"article_id\": \"2000000\",\n",
    "    \"subtopic_discussed\": \"1\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot - Topic B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>GPT4 Correct System: \n",
      "You are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article substantially discusses the subtopic \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\". \n",
      "Substantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \n",
      "\n",
      "The discussion on the subtopic \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\" may focus on one or more of the following aspects:\n",
      "\n",
      "- The governmental measures taken against the spread of the Coronavirus/COVID-19 pandemic, for example: social distancing, one and a half meter distance rule, lockdown, curfews, staying at home, quarantine, closing of restaurants, closing of shops, closing of schools & universities, travel restrictions, travel bans, border controls, guidelines imposed by different countries regarding domestic and international travel, measures effecting entertainment industry (closing of clubs, bars, rules about events and festivals), and other measures taken during the coronavirus pandemic.\n",
      "- Extensions or tightening of the measures taken during the Coronavirus/COVID-19 pandemic. \n",
      "- Precautionary measures and advice to combat the Coronavirus/COVID-19 pandemic, for example washing hands, wearing face masks, etc.\n",
      "- Relaxations (versoepelingen) and/or abolishing of the corona measures, for example: the end of the lockdown, the end of the curfew, the end of the one and a half meter distance rule, the end of the obligation to wear face masks, the end of the obligation to work from home, opening the restaurants, opening the shops, opening the schools & universities, lifting of travel restrictions, lifting of travel bans, lifting of border controls, guidelines imposed by different countries regarding domestic and international travel, measures effecting entertainment industry (opening of clubs, bars, rules about events and festivals).\n",
      "- Other discussions regarding the measures taken during the Coronavirus/COVID-19 pandemic, rules and regulations during the pandemic. \n",
      "\n",
      "<|end_of_turn|>GPT4 Correct User: \n",
      "Read the following article with the ID {article_id}: {text} \n",
      "\n",
      "This article falls under the categories: {category} and contains the keywords: {keywords}.\n",
      "\n",
      "Take a moment to understand the article. \n",
      "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
      "\n",
      "Carefully analyze if the article contains substantial discussion of \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\" based on the definition above.\n",
      "\n",
      "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
      "\n",
      "Output your results in a JSON format with keys 'article_id' and 'subtopic_discussed', where the id of the article and and your answer are the values. \n",
      "Follow the example output format provided. Do not include any additional information or explanation.\n",
      "\n",
      "Example Output (JSON format):\n",
      "{{\n",
      "    \"article_id\": \"2000000\",\n",
      "    \"subtopic_discussed\": \"1\"\n",
      "}}\n",
      "<|end_of_turn|>GPT4 Correct Assistant:\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt_b = zero_shot_prompt_messages(system_prompt, input_prompt, main_prompt)\n",
    "print(zero_shot_prompt_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"text\", \"category\", \"keywords\"],\n",
    "    template=zero_shot_prompt_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'category', 'keywords', 'text'], template='<s>GPT4 Correct System: \\nYou are a helpful AI assistant trained to analyze news articles. Your task is to determine whether the given news article substantially discusses the subtopic \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\". \\nSubstantial discussion means the article discusses one or more aspects of the subtopic in at least two sentences. \\n\\nThe discussion on the subtopic \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\" may focus on one or more of the following aspects:\\n\\n- The governmental measures taken against the spread of the Coronavirus/COVID-19 pandemic, for example: social distancing, one and a half meter distance rule, lockdown, curfews, staying at home, quarantine, closing of restaurants, closing of shops, closing of schools & universities, travel restrictions, travel bans, border controls, guidelines imposed by different countries regarding domestic and international travel, measures effecting entertainment industry (closing of clubs, bars, rules about events and festivals), and other measures taken during the coronavirus pandemic.\\n- Extensions or tightening of the measures taken during the Coronavirus/COVID-19 pandemic. \\n- Precautionary measures and advice to combat the Coronavirus/COVID-19 pandemic, for example washing hands, wearing face masks, etc.\\n- Relaxations (versoepelingen) and/or abolishing of the corona measures, for example: the end of the lockdown, the end of the curfew, the end of the one and a half meter distance rule, the end of the obligation to wear face masks, the end of the obligation to work from home, opening the restaurants, opening the shops, opening the schools & universities, lifting of travel restrictions, lifting of travel bans, lifting of border controls, guidelines imposed by different countries regarding domestic and international travel, measures effecting entertainment industry (opening of clubs, bars, rules about events and festivals).\\n- Other discussions regarding the measures taken during the Coronavirus/COVID-19 pandemic, rules and regulations during the pandemic. \\n\\n<|end_of_turn|>GPT4 Correct User: \\nRead the following article with the ID {article_id}: {text} \\n\\nThis article falls under the categories: {category} and contains the keywords: {keywords}.\\n\\nTake a moment to understand the article. \\nRemember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\\n\\nCarefully analyze if the article contains substantial discussion of \"The measures taken against the spread of the Coronavirus/COVID-19 pandemic, excluding measures related to testing and vaccines\" based on the definition above.\\n\\nAssign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\\n\\nOutput your results in a JSON format with keys \\'article_id\\' and \\'subtopic_discussed\\', where the id of the article and and your answer are the values. \\nFollow the example output format provided. Do not include any additional information or explanation.\\n\\nExample Output (JSON format):\\n{{\\n    \"article_id\": \"2000000\",\\n    \"subtopic_discussed\": \"1\"\\n}}\\n<|end_of_turn|>GPT4 Correct Assistant:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x77a9541f8ca0>), output_key='article_id, subtopic_discussed')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_b = LLMChain(llm = llm, prompt = prompt_template, output_key=\"article_id, subtopic_discussed\")\n",
    "chain_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "for index, row in unique_articles.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    text = row['Text']\n",
    "    category = row['Category']\n",
    "    keywords = row['Keywords']\n",
    "\n",
    "\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"text\": text,\n",
    "            \"category\": category,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_one.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot_b.append(generated_text)   \n",
    "\n",
    "# CPU times: user 2min 45s, sys: 9.83 s, total: 2min 55s\n",
    "# Wall time: 2min 55s\n",
    "# output cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = regex_extract_to_dataframe(generated_text_zeroshot_b)\n",
    "df_topic_b = pd.DataFrame(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan\n",
    "df_topic_b = df_topic_b.dropna()\n",
    "df_topic_b = df_topic_b.rename(columns={'topic_discussed': 'topic_b_pred'})\n",
    "df_topic_b['article_id'] = df_topic_b['article_id'].astype(int)\n",
    "df_topic_b['topic_b_pred'] = df_topic_b['topic_b_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elif = df[df['coder'] == 'Elif Kilik']\n",
    "df_coded_elif = df_elif[['article_id', 'topic_b']]\n",
    "df_zeroshot_merged_elif = pd.merge(df_coded_elif, df_topic_b, how='left', on=\"article_id\")\n",
    "df_zeroshot_merged_elif = df_zeroshot_merged_elif.dropna()\n",
    "df_zeroshot_merged_elif['topic_b_pred'] = df_zeroshot_merged_elif['topic_b_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        48\n",
      "           1       0.65      0.73      0.69        30\n",
      "\n",
      "    accuracy                           0.74        78\n",
      "   macro avg       0.73      0.74      0.74        78\n",
      "weighted avg       0.75      0.74      0.75        78\n",
      "\n",
      "Cohen's Kappa: 0.4715447154471545\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_zeroshot_merged_elif['topic_b'], df_zeroshot_merged_elif['topic_b_pred']))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(df_zeroshot_merged_elif['topic_b'], df_zeroshot_merged_elif['topic_b_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
