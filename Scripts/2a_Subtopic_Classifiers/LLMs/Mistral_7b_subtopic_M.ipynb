{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "\n",
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/\")\n",
    "\n",
    "huggingface_cache_dir = 'model_mistral'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "# set model access token for huggingface\n",
    "hf_token = 'hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map='auto',\n",
    "    token=hf_token,\n",
    "    cache_dir=huggingface_cache_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_token, cache_dir=huggingface_cache_dir)\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "Memory Usage: 13.488792419433594 GB\n",
      "Max Memory Usage: 13.488792419433594 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task='text-generation',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.0,  \n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # how much to avoid repeating the same word\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_extract_to_dataframe(strings):\n",
    "    # Initialize empty lists to store extracted values\n",
    "    article_ids = []\n",
    "    topic_discussed_values = []\n",
    "\n",
    "    # Define regex pattern for article_id and about_covid with optional double quotes\n",
    "    article_id_pattern = r'\"article_id\"\\s*:\\s*\"?(\\d+)\"?'\n",
    "    topic_discussed_pattern = r'\"subtopic_discussed\"\\s*:\\s*\"?(\\d)\"?'\n",
    "\n",
    "    # Iterate through each string\n",
    "    for string_data in strings:\n",
    "        # Clean the string_data: change \"\\\\_\" pattern to \"_\"\n",
    "        string_data = string_data.replace('\\\\_', '_',)\n",
    "\n",
    "        # Use regex to find matches for article_id\n",
    "        article_id_match = re.search(article_id_pattern, string_data)\n",
    "\n",
    "        # Use regex to find matches for about_covid\n",
    "        topic_discussed_match = re.search(topic_discussed_pattern, string_data)\n",
    "\n",
    "        # Extract values from the regex matches\n",
    "        article_id = int(article_id_match.group(1)) if article_id_match else None\n",
    "        topic_discussed = int(topic_discussed_match.group(1)) if topic_discussed_match else None\n",
    "\n",
    "        # Append values to the respective lists\n",
    "        article_ids.append(article_id)\n",
    "        topic_discussed_values.append(topic_discussed)\n",
    "\n",
    "    # Create a DataFrame using the extracted values\n",
    "    df = pd.DataFrame({\n",
    "        \"article_id\": article_ids,\n",
    "        \"subtopic_discussed\": topic_discussed_values\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt_messages(prompt_text):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    prompt = generate_text.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NOS articles annotated by the researcher\n",
    "df = pd.read_csv('NOS/nos_analysis/topic_tests_reliability_data/reliability_topics_elif.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)\n",
    "\n",
    "df = df[df['about_covid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df including text, category, keywords \n",
    "articles_df = pd.read_csv('NOS/nos_analysis/data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique articles from researcher annotations\n",
    "unique_articles = df[['article_id', 'Keywords', 'Category', 'Text']].drop_duplicates()\n",
    "print(unique_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Topic M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "As a helpful AI assistant, your task is to determine whether the news article after <<<>>> about the COVID-19 pandemic substantially discusses the subtopic \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\".\n",
    "Substantial discussion of a subtopic means that the article discusses one or more aspects of the subtopic in at least two sentences. \n",
    "\n",
    "Discussion of \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\" may focus on one or more of the following aspects:\n",
    "\n",
    "- Changes in political priorities and agendas in response to the coronavirus pandemic.\n",
    "- Parliamentary discussions about the coronavirus pandemic and coronavirus pandemic-related policies.\n",
    "- Allocation of government resources and investments in healthcare due to the coronavirus pandemic.\n",
    "- Re-evaluation of policy areas such as public health and crisis management.\n",
    "- Discussions about the effectiveness of political leaders' responses to the coronavirus pandemic.\n",
    "- Communication with the public during the crisis.\n",
    "- Accountability of governments and policymakers during the crisis.\n",
    "\n",
    "Read the following news article with the ID {article_id}: <<< {text} >>>\n",
    "\n",
    "This article falls under the categories: {category} and contains the keywords: {keywords}. \n",
    "\n",
    "Take a moment to understand the article. \n",
    "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
    "\n",
    "Carefully analyze if the article contains substantial discussion of \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\" based on the definition above.\n",
    " \n",
    "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
    "\n",
    "Your response should be in the following JSON format, without any additional explanations or notes:\n",
    "\n",
    "{{\n",
    "    \"article_id\": \"2000000\",\n",
    "  \"subtopic_discussed\": 0/1\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot - Topic M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] \n",
      "As a helpful AI assistant, your task is to determine whether the news article after <<<>>> about the COVID-19 pandemic substantially discusses the subtopic \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\".\n",
      "Substantial discussion of a subtopic means that the article discusses one or more aspects of the subtopic in at least two sentences. \n",
      "\n",
      "Discussion of \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\" may focus on one or more of the following aspects:\n",
      "\n",
      "- Changes in political priorities and agendas in response to the coronavirus pandemic.\n",
      "- Parliamentary discussions about the coronavirus pandemic and coronavirus pandemic-related policies.\n",
      "- Allocation of government resources and investments in healthcare due to the coronavirus pandemic.\n",
      "- Re-evaluation of policy areas such as public health and crisis management.\n",
      "- Discussions about the effectiveness of political leaders' responses to the coronavirus pandemic.\n",
      "- Communication with the public during the crisis.\n",
      "- Accountability of governments and policymakers during the crisis.\n",
      "\n",
      "Read the following news article with the ID {article_id}: <<< {text} >>>\n",
      "\n",
      "This article falls under the categories: {category} and contains the keywords: {keywords}. \n",
      "\n",
      "Take a moment to understand the article. \n",
      "Remember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\n",
      "\n",
      "Carefully analyze if the article contains substantial discussion of \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\" based on the definition above.\n",
      " \n",
      "Assign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\n",
      "\n",
      "Your response should be in the following JSON format, without any additional explanations or notes:\n",
      "\n",
      "{{\n",
      "    \"article_id\": \"2000000\",\n",
      "  \"subtopic_discussed\": 0/1\n",
      "}}\n",
      " [/INST]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = zero_shot_prompt_messages(prompt_text)\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"article_id\", \"text\", \"category\", \"keywords\"],\n",
    "    template=zero_shot_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['article_id', 'category', 'keywords', 'text'], template='<s> [INST] \\nAs a helpful AI assistant, your task is to determine whether the news article after <<<>>> about the COVID-19 pandemic substantially discusses the subtopic \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\".\\nSubstantial discussion of a subtopic means that the article discusses one or more aspects of the subtopic in at least two sentences. \\n\\nDiscussion of \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\" may focus on one or more of the following aspects:\\n\\n- Changes in political priorities and agendas in response to the coronavirus pandemic.\\n- Parliamentary discussions about the coronavirus pandemic and coronavirus pandemic-related policies.\\n- Allocation of government resources and investments in healthcare due to the coronavirus pandemic.\\n- Re-evaluation of policy areas such as public health and crisis management.\\n- Discussions about the effectiveness of political leaders\\' responses to the coronavirus pandemic.\\n- Communication with the public during the crisis.\\n- Accountability of governments and policymakers during the crisis.\\n\\nRead the following news article with the ID {article_id}: <<< {text} >>>\\n\\nThis article falls under the categories: {category} and contains the keywords: {keywords}. \\n\\nTake a moment to understand the article. \\nRemember, for a subtopic to be substantially discussed, the article must discuss one or more aspects of the subtopic in at least two sentences.\\n\\nCarefully analyze if the article contains substantial discussion of \"Political discussions about the coronavirus/COVID-19 pandemic and the effects of the pandemic on politics\" based on the definition above.\\n \\nAssign a value of 1 if the article substantially discusses the subtopic, and a value of 0 if the article does not substantially discuss the subtopic.\\n\\nYour response should be in the following JSON format, without any additional explanations or notes:\\n\\n{{\\n    \"article_id\": \"2000000\",\\n  \"subtopic_discussed\": 0/1\\n}}\\n [/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x71b6c82cd7c0>), output_key='article_id, subtopic_discussed')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_one = LLMChain(llm = llm, prompt = prompt_template, output_key=\"article_id, subtopic_discussed\")\n",
    "chain_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_zeroshot_m = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "for index, row in unique_articles.iterrows():  \n",
    "    article_id = row['article_id']\n",
    "    text = row['Text']\n",
    "    category = row['Category']\n",
    "    keywords = row['Keywords']\n",
    "\n",
    "\n",
    "    input_variables = {\n",
    "            \"article_id\": article_id,\n",
    "            \"text\": text,\n",
    "            \"category\": category,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_one.run(input_variables)\n",
    "    print(generated_text)\n",
    "    generated_text_zeroshot_m.append(generated_text)     \n",
    "\n",
    "# CPU times: user 2min 43s, sys: 4.63 s, total: 2min 47s\n",
    "# Wall time: 2min 47s\n",
    "# output cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = regex_extract_to_dataframe(generated_text_zeroshot_m)\n",
    "df_topic_m = pd.DataFrame(json_list)\n",
    "# drop nan\n",
    "df_topic_m = df_topic_m.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elif = df[df['coder'] == 'Elif Kilik']\n",
    "df_coded_elif = df_elif[['article_id', 'topic_m']]\n",
    "df_zeroshot_merged_elif = pd.merge(df_coded_elif, df_topic_m, how='left', on=\"article_id\")\n",
    "df_zeroshot_merged_elif = df_zeroshot_merged_elif.dropna()\n",
    "df_zeroshot_merged_elif['topic_m_pred'] = df_zeroshot_merged_elif['topic_m_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89        72\n",
      "           1       0.20      0.50      0.29         6\n",
      "\n",
      "    accuracy                           0.81        78\n",
      "   macro avg       0.58      0.67      0.59        78\n",
      "weighted avg       0.89      0.81      0.84        78\n",
      "\n",
      "Cohen's Kappa: 0.19753086419753096\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_zeroshot_merged_elif['topic_m'], df_zeroshot_merged_elif['topic_m_pred']))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(df_zeroshot_merged_elif['topic_m'], df_zeroshot_merged_elif['topic_m_pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
