{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import set_seed, AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.metrics import classification_report\n",
    "from ftfy import fix_text\n",
    "\n",
    "import os\n",
    "# os.getcwd()\n",
    "\n",
    "huggingface_cache_dir = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('actors_training_df_Robbert.xlsx', engine=\"openpyxl\")\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input_text\"] = df[\"input_text\"].apply(fix_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FPÖ: De heftigste kritiek uit politieke hoek kwam van Herbert Kickl, voorzitter van de rechts-populistische oppositiepartij FPÖ. \\nVorige week riep de FPÖ al op tot een demonstratie, morgen.',\n",
       "       'Herbert Kickl: De heftigste kritiek uit politieke hoek kwam van Herbert Kickl, voorzitter van de rechts-populistische oppositiepartij FPÖ.',\n",
       "       'FPÖ: Tot de demonstratie was mede opgeroepen door de rechtse politieke partij FPÖ.',\n",
       "       'AFP: Kluge, die directeur Europa van de WHO is, zegt in een interview met het Franse persbureau AFP dat \"wanneer omikron eenmaal verdwijnt er voor heel wat weken en maanden een periode van wijdverbreide immuniteit zal zijn\".',\n",
       "       'Hans Kluge: Dat zegt de Belg Hans Kluge van de Wereldgezondheidsorganisatie (WHO) over de coronapandemie in Europa. \\nKluge, die directeur Europa van de WHO is, zegt in een interview met het Franse persbureau AFP dat \"wanneer omikron eenmaal verdwijnt er voor heel wat weken en maanden een periode van wijdverbreide immuniteit zal zijn\". \\nWHO-directeur Kluge over de nabije toekomst van de pandemie in Europa:\\n Die immuniteit is volgens Kluge te danken aan de vaccinaties en de vele besmettingen. \\nKluge verwacht dat tegen maart ongeveer 60 procent van de Europeanen een besmetting met de omikronvariant heeft doorgemaakt.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see some examples of input text\n",
    "df['input_text'].values[:34]\n",
    "# df[\"input_text\"] = df[\"input_text\"].replace({\"√∂\": \"ö\", \"√´\": \"é\", \"√º\": \"ü\"}, regex=True)\n",
    "\n",
    "# see some examples of inpyut text where string contains word patienten with e with dots\n",
    "df[df['input_text'].str.contains('FP')]['input_text'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the first : with :\\n\n",
    "df[\"input_text\"] = df[\"input_text\"].str.replace(\":\", \":\\n\", n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the reliability df\n",
    "reliability_df = pd.read_csv('NOS/nos_analysis/actor_analysis/coref_resolution/reliability_actors_final_cleaned_elif.csv',\n",
    "                             sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "reliability_df['article_id'] = reliability_df['article_id'].astype(int)\n",
    "reliability_df = reliability_df[reliability_df['coder'] == 'Elif Kilik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[~df['article_id'].isin(reliability_df['article_id'])]\n",
    "test_df = df[df['article_id'].isin(reliability_df['article_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoted\n",
       "1    1326\n",
       "0     813\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.quoted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elifk\\AppData\\Local\\Temp\\ipykernel_35848\\65960959.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['token_size'] = train_df['input_text'].apply(lambda x: len(x.split()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2139.000000\n",
       "mean       51.587190\n",
       "std        45.841842\n",
       "min         5.000000\n",
       "25%        23.000000\n",
       "50%        37.000000\n",
       "75%        64.000000\n",
       "max       428.000000\n",
       "Name: token_size, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the token size for input_text values\n",
    "train_df['token_size'] = train_df['input_text'].apply(lambda x: len(x.split()))\n",
    "train_df['token_size'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925, 1) (214, 1)\n",
      "(1925, 1) (214, 1)\n"
     ]
    }
   ],
   "source": [
    "X = train_df[['input_text']]\n",
    "y = train_df[['quoted']]\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape)  # Shapes of the input texts\n",
    "print(y_train.shape, y_val.shape)    # Shapes of the binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoted\n",
       "1    1191\n",
       "0     734\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.quoted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoted\n",
       "1    135\n",
       "0     79\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()\n",
    "y_val.quoted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elifk\\miniconda3\\envs\\llmenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for 3 epochs...\n",
      "Epoch: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:03<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2248\n",
      "New best model found! Saving...\n",
      "Model and optimizer states saved.\n",
      "Epoch: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2318\n",
      "Epoch: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:20<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3120\n",
      "\n",
      "Training for 5 epochs...\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4241\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:22<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4166\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:20<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:03<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.8014\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with learning rate: 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for 3 epochs...\n",
      "Epoch: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5787\n",
      "Epoch: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:03<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4444\n",
      "Epoch: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:20<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3396\n",
      "Early stopping triggered.\n",
      "\n",
      "Training for 5 epochs...\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3454\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:22<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3189\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:20<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2679\n",
      "Early stopping triggered.\n",
      "\n",
      "Training with learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for 3 epochs...\n",
      "Epoch: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:22<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2995\n",
      "Epoch: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:21<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2624\n",
      "Epoch: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2361\n",
      "Early stopping triggered.\n",
      "\n",
      "Training for 5 epochs...\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:22<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2471\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:19<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3327\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 241/241 [01:20<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3585\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [1e-5, 1e-6, 5e-6]\n",
    "accumulation_steps = 2  # This simulates a batch size of 16 by accumulating over 2 steps with batch size 8\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 3  # Number of epochs to wait before stopping if no improvement\n",
    "epochs_to_test = [3, 5]  # Epochs to test\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", cache_dir=huggingface_cache_dir)\n",
    "\n",
    "# Encode data\n",
    "def encode(docs):\n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, padding='max_length',\n",
    "                                                return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "train_input_ids, train_att_masks = encode(X_train['input_text'].tolist())\n",
    "valid_input_ids, valid_att_masks = encode(X_val['input_text'].tolist())\n",
    "train_y = torch.LongTensor(y_train.values.squeeze())\n",
    "valid_y = torch.LongTensor(y_val.values.squeeze())\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=8)  \n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=8)\n",
    "\n",
    "save_directory = \"your_save_directory\"  # Specify your save directory here\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Loop through specified learning rates\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Initialize the model for each learning rate\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", num_labels=2,  # Binary classification\n",
    "                                                               cache_dir=huggingface_cache_dir)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Loop through specified epochs\n",
    "    for epochs in epochs_to_test:\n",
    "        print(f\"\\nTraining for {epochs} epochs...\")\n",
    "        patience_counter = 0  # To track epochs without improvement\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            print(f\"Epoch: {epoch_num + 1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "                input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                output = model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "                loss = output.loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss = loss / accumulation_steps  # Scale the loss\n",
    "                loss.backward()  # Backpropagate the loss\n",
    "\n",
    "                # Gradient accumulation\n",
    "                if (step_num + 1) % accumulation_steps == 0:\n",
    "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            # Average training loss for this epoch\n",
    "            train_loss /= len(train_dataloader)\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            valid_pred = []\n",
    "            with torch.no_grad():\n",
    "                for step_num_e, batch_data in enumerate(tqdm(valid_dataloader, desc='Validation')):\n",
    "                    input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                    output = model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "                    loss = output.loss\n",
    "                    valid_loss += loss.item()\n",
    "                    valid_pred.append(output.logits.cpu().detach().numpy())\n",
    "\n",
    "            # Average validation loss for this epoch\n",
    "            valid_loss /= len(valid_dataloader)\n",
    "            print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                patience_counter = 0  # Reset patience counter if we have improvement\n",
    "                print(\"New best model found! Saving...\")\n",
    "\n",
    "                # Save the model components\n",
    "                torch.save(best_model_state, os.path.join(save_directory, \"best_model_state.bin\"))\n",
    "                tokenizer.save_pretrained(save_directory)\n",
    "                model.config.save_pretrained(save_directory)\n",
    "                torch.save(optimizer.state_dict(), os.path.join(save_directory, \"optimizer_state.bin\"))\n",
    "                print(\"Model and optimizer states saved.\")\n",
    "\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break  # Stop training if no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91        79\n",
      "           1       0.92      0.98      0.95       135\n",
      "\n",
      "    accuracy                           0.93       214\n",
      "   macro avg       0.94      0.92      0.93       214\n",
      "weighted avg       0.94      0.93      0.93       214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "valid_loss = 0\n",
    "valid_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num_e, batch_data in enumerate(tqdm(valid_dataloader, desc='Validation')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        # During validation, no need to pass labels to the model\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        logits = output.logits\n",
    "        \n",
    "        # Compute validation loss manually (if needed)\n",
    "        # loss = criterion(logits, labels)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # Store the logits for all validation examples\n",
    "        valid_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "# Average validation loss\n",
    "valid_loss /= len(valid_dataloader)\n",
    "print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "# Check if valid_pred has any entries before concatenation\n",
    "if valid_pred:\n",
    "    valid_pred = np.concatenate(valid_pred)  # Concatenate predictions from batches\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    valid_pred_softmax = torch.softmax(torch.tensor(valid_pred), dim=1).numpy()\n",
    "\n",
    "    # Get the class predictions (0 or 1) based on the higher probability\n",
    "    valid_pred_labels = np.argmax(valid_pred_softmax, axis=1)\n",
    "\n",
    "    # Flatten ground truth for classification report\n",
    "    y_true = valid_y.numpy()\n",
    "\n",
    "    print(classification_report(y_true, valid_pred_labels, \n",
    "                                zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\elifk\\AppData\\Local\\Temp\\ipykernel_35848\\3788387349.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(save_directory, \"best_model_state.bin\")))\n",
      "c:\\Users\\elifk\\miniconda3\\envs\\llmenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\elifk\\AppData\\Local\\Temp\\ipykernel_35848\\3788387349.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(os.path.join(save_directory, \"optimizer_state.bin\")))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# Load the model architecture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", num_labels=2, cache_dir=huggingface_cache_dir)\n",
    "\n",
    "# Load the saved model state (weights)\n",
    "model.load_state_dict(torch.load(os.path.join(save_directory, \"best_model_state.bin\")))\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model configuration\n",
    "model.config.from_pretrained(save_directory)\n",
    "\n",
    "# Load the optimizer state (if you plan to resume training)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer.load_state_dict(torch.load(os.path.join(save_directory, \"optimizer_state.bin\")))\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)  # Saves the model architecture and weights\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)  # Saves the tokenizer\n",
    "\n",
    "# Optionally, save the optimizer state if you're resuming training\n",
    "torch.save(optimizer.state_dict(), os.path.join(save_directory, \"optimizer_state.bin\"))\n",
    "\n",
    "# Set model to evaluation mode for inference\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(save_directory)  # Saves the model architecture and weights\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)  # Saves the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 61/61 [00:06<00:00,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2689\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       214\n",
      "           1       0.91      0.92      0.92       271\n",
      "\n",
      "    accuracy                           0.91       485\n",
      "   macro avg       0.90      0.90      0.90       485\n",
      "weighted avg       0.91      0.91      0.91       485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure that you have the device set up\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# encode input text for the test set\n",
    "test_input_ids, test_att_masks = encode(test_df['input_text'].tolist())\n",
    "test_y = torch.LongTensor(test_df['quoted'].values.squeeze())\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)\n",
    "\n",
    "# Initialize loss and predictions\n",
    "test_loss = 0\n",
    "test_pred = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    for batch_data in tqdm(test_dataloader, desc='Testing'):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "\n",
    "        # Get model output\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "        loss = output.loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store logits for prediction\n",
    "        test_pred.append(output.logits.cpu().detach().numpy())\n",
    "\n",
    "# Compute average loss\n",
    "test_loss /= len(test_dataloader)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Ensure there are predictions to evaluate\n",
    "if test_pred:\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "\n",
    "    # Apply softmax for class probabilities\n",
    "    test_pred_softmax = torch.softmax(torch.tensor(test_pred).cpu(), dim=1).numpy()\n",
    "\n",
    "    # Get predicted labels\n",
    "    test_pred_labels = np.argmax(test_pred_softmax, axis=1)\n",
    "\n",
    "    # Get true labels\n",
    "    y_true = test_y.numpy()\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_true, test_pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the predictions to the df_elif\n",
    "test_df['predicted'] = test_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df_elif\n",
    "test_df.to_csv('path_to_RobBERT_quote_classifier.csv',\n",
    "               sep = ';', encoding = 'utf-8', index = False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
