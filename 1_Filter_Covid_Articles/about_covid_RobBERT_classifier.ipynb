{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go one level up in the directory\n",
    "os.chdir(\"/data/500gbstorage/actor_classification\")\n",
    "\n",
    "\n",
    "huggingface_cache_dir = 'model'\n",
    "\n",
    "# change huggingface cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = huggingface_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the manually annotated dataset\n",
    "df = pd.read_csv('data/coded_df_topics_full.csv',\n",
    "                 sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# change article_id to integer\n",
    "df['article_id'] = df['article_id'].astype(int)\n",
    "\n",
    "topic_vars = ['about_covid',  'topic_a', 'topic_b', 'topic_c', 'topic_d', 'topic_e', 'topic_f', 'topic_g', 'topic_h', \n",
    "              'topic_i', 'topic_j', 'topic_k', 'topic_l', 'topic_m', 'topic_n']\n",
    "\n",
    "# change all topic vars to int\n",
    "for i in topic_vars:\n",
    "    df[i] = df[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles df includes all article texts, categories and keywords\n",
    "articles_df = pd.read_csv('data/final_nosarticles.csv',\n",
    "                          sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(articles_df.shape)\n",
    "\n",
    "# get article text, category, keywords and page_id\n",
    "articles_df = articles_df[['page_id', 'Title', 'Text', 'Category', 'Keywords']].drop_duplicates()\n",
    "# make page id integer\n",
    "articles_df['page_id'] = articles_df['page_id'].astype(int)\n",
    "# change page_id to article_id\n",
    "articles_df.rename(columns = {'page_id': 'article_id'}, inplace = True)\n",
    "# change LINE BREAK to \\n\n",
    "articles_df['Text'] = articles_df['Text'].str.replace('[LINE_BREAK]', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an input text, combining Title, Text, Category and Keywords and before Category add string 'Categories: ' and before Keywords add string 'Keywords: ' if Category and Keywords are empty skip them\n",
    "articles_df['Category'] = articles_df['Category'].fillna('')\n",
    "articles_df['Keywords'] = articles_df['Keywords'].fillna('')\n",
    "articles_df['input_text'] = articles_df['Text'] + '\\n' + 'Categories: ' + articles_df['Category'] + ' ' + 'Keywords: ' + articles_df['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles_df with df\n",
    "df = pd.merge(df, articles_df, on='article_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the reliability annotations/test data\n",
    "train_df = df[df['reliability_article'] == 0]\n",
    "test_df = df[df['reliability_article'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "about_covid\n",
       "1    410\n",
       "0    266\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.about_covid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 1) (136, 1)\n",
      "(540, 1) (136, 1)\n"
     ]
    }
   ],
   "source": [
    "# Select input text\n",
    "X = train_df[['input_text']]\n",
    "\n",
    "# Select target labels directly from the binary topic columns\n",
    "y = train_df[['about_covid']]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape)  # Shapes of the input texts\n",
    "print(y_train.shape, y_val.shape)    # Shapes of the binary labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6435/231938727.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['num_tokens'] = train_df['input_text'].apply(count_tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     676.000000\n",
       "mean      450.272189\n",
       "std       210.423411\n",
       "min        91.000000\n",
       "25%       276.000000\n",
       "50%       404.000000\n",
       "75%       611.000000\n",
       "max      1106.000000\n",
       "Name: num_tokens, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of tokens in the input text\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "train_df['num_tokens'] = train_df['input_text'].apply(count_tokens)\n",
    "train_df['num_tokens'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned RobBERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [2e-5, 2e-6]\n",
    "accumulation_steps = 4  # This simulates a batch size of 16 by accumulating over 2 steps with batch size 8\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 4  # Number of epochs to wait before stopping if no improvement\n",
    "epochs_to_test = [5, 10]  # Epochs to test\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", cache_dir=huggingface_cache_dir)\n",
    "\n",
    "# Encode data\n",
    "def encode(docs):\n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, padding='max_length',\n",
    "                                                return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "train_input_ids, train_att_masks = encode(X_train['input_text'].tolist())\n",
    "valid_input_ids, valid_att_masks = encode(X_val['input_text'].tolist())\n",
    "train_y = torch.LongTensor(y_train.values.squeeze())\n",
    "valid_y = torch.LongTensor(y_val.values.squeeze())\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=4)  # Small batch size\n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=4)\n",
    "\n",
    "# Loop through specified learning rates\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Initialize the model for each learning rate\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\", \n",
    "                                                                num_labels=2, \n",
    "                                                                cache_dir=huggingface_cache_dir)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Loop through specified epochs\n",
    "    for epochs in epochs_to_test:\n",
    "        print(f\"\\nTraining for {epochs} epochs...\")\n",
    "        patience_counter = 0  # To track epochs without improvement\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            print(f\"Epoch: {epoch_num + 1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "                input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                \n",
    "                # CrossEntropyLoss automatically applies softmax, so logits are passed directly\n",
    "                output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "                logits = output.logits  # Get logits directly\n",
    "                \n",
    "                loss = criterion(logits, labels)  # Compute loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss = loss / accumulation_steps  # Scale the loss\n",
    "                loss.backward()  # Backpropagate the loss\n",
    "\n",
    "                # Gradient accumulation\n",
    "                if (step_num + 1) % accumulation_steps == 0:\n",
    "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            # Average training loss for this epoch\n",
    "            train_loss /= len(train_dataloader)\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            valid_pred = []\n",
    "            with torch.no_grad():\n",
    "                for step_num_e, batch_data in enumerate(tqdm(valid_dataloader, desc='Validation')):\n",
    "                    input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                    output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "                    logits = output.logits\n",
    "                    \n",
    "                    loss = criterion(logits, labels)  # Compute loss\n",
    "                    valid_loss += loss.item()\n",
    "                    valid_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "            # Average validation loss for this epoch\n",
    "            valid_loss /= len(valid_dataloader)\n",
    "            print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                patience_counter = 0  # Reset patience counter if we have improvement\n",
    "                print(\"New best model found! Saving...\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break  # Stop training if no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with Val loss: 0.2772\n"
     ]
    }
   ],
   "source": [
    "# After training, save the best model\n",
    "torch.save(best_model_state, 'results/about_covid_robbert_base_best_model.pt')\n",
    "print(f\"Best model saved with Val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DTAI-KULeuven/robbert-2023-dutch-base\",\n",
    "                                                              num_labels=2, cache_dir=huggingface_cache_dir)\n",
    "\n",
    "model.load_state_dict(torch.load('results/about_covid_robbert_base_best_model.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:01<00:00, 31.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4954\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "not_about_covid       0.96      0.84      0.90        57\n",
      "    about_covid       0.90      0.97      0.93        79\n",
      "\n",
      "       accuracy                           0.92       136\n",
      "      macro avg       0.93      0.91      0.92       136\n",
      "   weighted avg       0.92      0.92      0.92       136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "valid_loss = 0\n",
    "valid_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num_e, batch_data in enumerate(tqdm(valid_dataloader, desc='Validation')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        # During validation, no need to pass labels to the model\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        logits = output.logits\n",
    "        \n",
    "        # Compute validation loss manually (if needed)\n",
    "        loss = criterion(logits, labels)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # Store the logits for all validation examples\n",
    "        valid_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "# Average validation loss\n",
    "valid_loss /= len(valid_dataloader)\n",
    "print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "# Check if valid_pred has any entries before concatenation\n",
    "if valid_pred:\n",
    "    valid_pred = np.concatenate(valid_pred)  # Concatenate predictions from batches\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    valid_pred_softmax = torch.softmax(torch.tensor(valid_pred), dim=1).numpy()\n",
    "\n",
    "    # Get the class predictions (0 or 1) based on the higher probability\n",
    "    valid_pred_labels = np.argmax(valid_pred_softmax, axis=1)\n",
    "\n",
    "    # Flatten ground truth for classification report\n",
    "    y_true = valid_y.numpy()\n",
    "\n",
    "    print(classification_report(y_true, valid_pred_labels, \n",
    "                                target_names=['not_about_covid', 'about_covid'],\n",
    "                                zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test annotations annotated by the researcher\n",
    "reliability_df = pd.read_csv('data/reliability_topics_elif.csv',\n",
    "                             sep = ';', encoding = 'utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "reliability_df['article_id'] = reliability_df['article_id'].astype(int)\n",
    "\n",
    "reliability_df.head()\n",
    "\n",
    "# merge reliability_df with articles_df\n",
    "reliability_df = pd.merge(reliability_df, articles_df, on='article_id', how = 'left')\n",
    "print(reliability_df.shape)\n",
    "\n",
    "# make an input text, combining Title, Text, Category and Keywords and before Category add string 'Categories: ' and before Keywords add string 'Keywords: ' if Category and Keywords are empty skip them\n",
    "reliability_df['Category'] = reliability_df['Category'].fillna('')\n",
    "reliability_df['Keywords'] = reliability_df['Keywords'].fillna('')\n",
    "reliability_df['input_text'] = reliability_df['Text'] + '\\n' + 'Categories: ' + reliability_df['Category'] + ' ' + 'Keywords: ' + reliability_df['Keywords']\n",
    "\n",
    "# make a test_df for all coders separately\n",
    "test_df_elif = reliability_df[reliability_df['coder'] == 'Elif Kilik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 30/30 [00:00<00:00, 31.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "not_about_covid       0.89      0.81      0.85        42\n",
      "    about_covid       0.90      0.95      0.93        78\n",
      "\n",
      "       accuracy                           0.90       120\n",
      "      macro avg       0.90      0.88      0.89       120\n",
      "   weighted avg       0.90      0.90      0.90       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run model on the test data\n",
    "\n",
    "X_test = test_df_elif[['input_text']]\n",
    "# Select target labels directly from the binary topic columns\n",
    "y_test = test_df_elif[['about_covid']]\n",
    "# Encode data\n",
    "test_input_ids, test_att_masks = encode(X_test['input_text'].tolist())\n",
    "test_y = torch.LongTensor(y_test.values.squeeze())\n",
    "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=4)\n",
    "\n",
    "# predict on test data\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num_e, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        # During validation, no need to pass labels to the model\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask)\n",
    "        logits = output.logits\n",
    "        \n",
    "        # Compute validation loss manually (if needed)\n",
    "        loss = criterion(logits, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store the logits for all validation examples\n",
    "        test_pred.append(logits.cpu().detach().numpy())\n",
    "\n",
    "# get predictions to compute classification report\n",
    "\n",
    "# Average validation loss\n",
    "test_loss /= len(test_dataloader)\n",
    "\n",
    "# Check if valid_pred has any entries before concatenation\n",
    "\n",
    "if test_pred:\n",
    "    test_pred = np.concatenate(test_pred)  # Concatenate predictions from batches\n",
    "\n",
    "    # Apply softmax to get class probabilities\n",
    "    test_pred_softmax = torch.softmax(torch.tensor(test_pred), dim=1).numpy()\n",
    "\n",
    "    # Get the class predictions\n",
    "    test_pred_labels = np.argmax(test_pred_softmax, axis=1)\n",
    "\n",
    "    # Flatten ground truth for classification report\n",
    "    y_true = test_y.numpy()\n",
    "    \n",
    "    print(classification_report(y_true, test_pred_labels,\n",
    "                                target_names=['not_about_covid', 'about_covid'],\n",
    "                                zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistralenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
